import pandas as pd
import numpy as np

# Set a seed for reproducibility
np.random.seed(42)

print("--- Starting Data Generation ---")

# =======================================================
# 1. Generate Pet Profiles (The Items)
# =======================================================
num_pets = 1000

pet_data = {
    'Pet_ID': [f'P{i:04d}' for i in range(1, num_pets + 1)],
    'Type': np.random.choice(['Dog', 'Cat'], size=num_pets, p=[0.6, 0.4]),
    'Age_Group': np.random.choice(['Puppy/Kitten', 'Young_Adult', 'Senior'], size=num_pets, p=[0.3, 0.5, 0.2]),
    'Size': np.random.choice(['Small', 'Medium', 'Large'], size=num_pets, p=[0.35, 0.45, 0.2]),
    # Energy Level: 1=Low, 2=Medium, 3=High (Matches Adopter Activity)
    'Energy_Level': np.random.choice([1, 2, 3], size=num_pets, p=[0.3, 0.4, 0.3]),
    # Good_With_Kids: 0=No, 1=Yes, 2=Needs older kids (Matches Adopter Has_Kids)
    'Good_With_Kids': np.random.choice([0, 1, 2], size=num_pets, p=[0.1, 0.7, 0.2]),
    'Grooming_Needs': np.random.choice(['Low', 'Medium', 'High'], size=num_pets, p=[0.5, 0.3, 0.2]),
}

pet_profiles = pd.DataFrame(pet_data)
pet_profiles.to_csv('pet_profiles.csv', index=False)
print(f"Generated and saved pet_profiles.csv with {len(pet_profiles)} records.")


# =======================================================
# 2. Generate Adopter Profiles (The Users)
# =======================================================
num_adopters = 500

adopter_data = {
    'Adopter_ID': [f'U{i:03d}' for i in range(1, num_adopters + 1)],
    'Housing_Type': np.random.choice(['Apartment', 'House_No_Yard', 'House_Yard'], size=num_adopters, p=[0.4, 0.3, 0.3]),
    'Has_Kids': np.random.choice([0, 1], size=num_adopters, p=[0.6, 0.4]),
    # Time_At_Home: 1=Away_Most_Day, 2=Hybrid, 3=WFH_Full_Time
    'Time_At_Home': np.random.choice([1, 2, 3], size=num_adopters, p=[0.25, 0.4, 0.35]),
    # Activity Level: 1=Low, 2=Medium, 3=High (Matches Pet Energy)
    'Activity_Level': np.random.choice([1, 2, 3], size=num_adopters, p=[0.3, 0.4, 0.3]),
    'Experience_Level': np.random.choice(['First_Time', 'Past_Owner', 'Expert'], size=num_adopters, p=[0.3, 0.5, 0.2]),
    'Pet_Type_Desired': np.random.choice(['Dog', 'Cat'], size=num_adopters, p=[0.7, 0.3]),
}

adopter_profiles = pd.DataFrame(adopter_data)
adopter_profiles.to_csv('adopter_profiles.csv', index=False)
print(f"Generated and saved adopter_profiles.csv with {len(adopter_profiles)} records.")


# =======================================================
# 3. Generate Historical Matches (The Target Variable) - CORRECTED
# =======================================================
# (The first part of step 3 remains the same)
num_attempts = 3000
historical_matches = pd.DataFrame({
    'Adopter_ID': np.random.choice(adopter_profiles['Adopter_ID'], size=num_attempts),
    'Pet_ID': np.random.choice(pet_profiles['Pet_ID'], size=num_attempts),
})

# Merge features for rule creation. (Removing suffixes= because we will use the base names)
history = pd.merge(historical_matches, adopter_profiles, on='Adopter_ID')
history = pd.merge(history, pet_profiles, on='Pet_ID')

# Initialize the target variable
history['Success_Match'] = 0

# Define Success/Failure Logic (The patterns the ML model must learn)
# --- CORRECTED COLUMN NAMES USED BELOW (using base names) ---

# Rule 1: Energy Match (HIGH Success Probability)
# Adopter's Activity_Level should match Pet's Energy_Level
history.loc[history['Activity_Level'] == history['Energy_Level'], 'Success_Match'] = 1

# Rule 2: Apartment/Large Pet Mismatch (HIGH Failure Probability)
# Adopter in Apartment adopts a Large pet -> Failure (Target = 0)
history.loc[(history['Housing_Type'] == 'Apartment') & (history['Size'] == 'Large'), 'Success_Match'] = 0

# Rule 3: Kids/Pet Compatibility Mismatch (HIGH Failure Probability)
# Adopter has kids (1) but pet is explicitly not good with kids (0) -> Failure
history.loc[(history['Has_Kids'] == 1) & (history['Good_With_Kids'] == 0), 'Success_Match'] = 0

# Rule 4: Experienced Owner/High Needs Match (Positive boost)
# Experienced owner and high grooming needs -> Success
history.loc[(history['Experience_Level'] == 'Expert') & (history['Grooming_Needs'] == 'High'), 'Success_Match'] = 1

# Add random noise/realism (10% of attempts)
noise_count = int(num_attempts * 0.1)
history.loc[history.sample(noise_count).index, 'Success_Match'] = np.random.choice([0, 1], size=noise_count)


# Finalize the historical matches file
historical_matches_final = history[['Adopter_ID', 'Pet_ID', 'Success_Match']].drop_duplicates()
historical_matches_final.to_csv('historical_matches.csv', index=False)
print(f"Generated and saved historical_matches.csv with {len(historical_matches_final)} records (The Training Data).")


import pandas as pd
import numpy as np

# Set a seed for reproducibility
np.random.seed(42)

# --- 1. Load the generated data ---
# Note: We load the full history dataframe which contains merged features for convenience.
history = pd.read_csv('historical_matches.csv') # This file only has ID and Success_Match

# Re-run the merge logic to get all features back for ML prep
adopter_profiles = pd.read_csv('adopter_profiles.csv')
pet_profiles = pd.read_csv('pet_profiles.csv')

history = pd.merge(history, adopter_profiles, on='Adopter_ID')
history = pd.merge(history, pet_profiles, on='Pet_ID')

print("--- Preparing Training Datasets for 6 ML Algorithms ---")

# =======================================================
# 1. Decision Tree (DT) - Pawsonality Classification
# Purpose: Classify user quiz answers into 8 Pawsonality types.
# Target Variable: Target_Pawsonality (8 Categories)
# =======================================================

dt_data = adopter_profiles.copy()

def classify_pawsonality(row):
    # Active Adventurer: High Activity (3) & Experienced/Expert
    if row['Activity_Level'] == 3 and row['Experience_Level'] in ['Past_Owner', 'Expert']:
        return 'Active Adventurer'
    # Cozy Companion: Low Activity (1) & Apartment (low energy life)
    elif row['Activity_Level'] == 1 and row['Housing_Type'] == 'Apartment':
        return 'Cozy Companion'
    # Playful Enthusiast: High Activity (3) & Young/Hybrid Time
    elif row['Activity_Level'] == 3 and row['Time_At_Home'] in [1, 2]:
        return 'Playful Enthusiast'
    # Confident Guardian: Expert Experience & House_Yard (potential for large breeds)
    elif row['Experience_Level'] == 'Expert' and row['Housing_Type'] == 'House_Yard':
        return 'Confident Guardian'
    # Gentle Nurturer: First_Time/Past_Owner & WFH_Full_Time (more time for special needs)
    elif row['Experience_Level'] in ['First_Time', 'Past_Owner'] and row['Time_At_Home'] == 3:
        return 'Gentle Nurturer'
    # Quiet Caretaker: Low Activity (1) & Hybrid/Away_Most_Day (independent pets)
    elif row['Activity_Level'] == 1 and row['Time_At_Home'] in [1, 2]:
        return 'Quiet Caretaker'
    # Social Butterfly: Medium Activity (2) & Has_Kids (1) (busy household)
    elif row['Activity_Level'] == 2 and row['Has_Kids'] == 1:
        return 'Social Butterfly'
    # Balanced Buddy (Default/Adaptable)
    else:
        return 'Balanced Buddy'

dt_data['Target_Pawsonality'] = dt_data.apply(classify_pawsonality, axis=1)

# Features for DT: Housing_Type, Has_Kids, Time_At_Home, Activity_Level, Experience_Level
dt_training_data = dt_data[['Adopter_ID', 'Housing_Type', 'Has_Kids', 'Time_At_Home', 'Activity_Level', 'Experience_Level', 'Target_Pawsonality']]
dt_training_data.to_csv('1_dt_pawsonality_training.csv', index=False)
print("1. Decision Tree (Pawsonality) Training Data saved.")

# =======================================================
# 2 & 5. SVM & ANN - Core Matching & Deep Matching
# Purpose: Calculate compatibility score (Success_Match is the binary target).
# Target Variable: Success_Match (0/1)
# =======================================================

# Add a derived feature: Absolute difference in Activity/Energy
history['Activity_Energy_Diff'] = np.abs(history['Activity_Level'] - history['Energy_Level'])

# Encode categorical features needed for SVM/ANN (Dummy variables)
match_features = history[['Adopter_ID', 'Pet_ID', 'Activity_Energy_Diff', 'Has_Kids', 'Good_With_Kids', 'Experience_Level', 'Grooming_Needs', 'Size', 'Success_Match']]
match_training_data = pd.get_dummies(match_features, columns=['Experience_Level', 'Grooming_Needs', 'Size'], drop_first=True)

# Select final columns for SVM/ANN training
svm_ann_training_data = match_training_data.drop(columns=['Adopter_ID', 'Pet_ID'])

# The Success_Match column is the target for both
svm_ann_training_data.to_csv('2_5_svm_ann_matching_training.csv', index=False)
print("2 & 5. SVM/ANN (Matching) Training Data saved.")

# =======================================================
# 3. Naive Bayes (NB) - Auto-Tagging
# Purpose: Classify pet descriptions into filter tags.
# Feature: Pet_Description (requires text generation)
# Target Variable: Pet_Tags (Multi-label)
# =======================================================
nb_data = pet_profiles.copy()

def generate_description(row):
    desc = f"A {row['Age_Group'].lower().replace('_', ' ')} {row['Type'].lower()} with {row['Grooming_Needs'].lower()} grooming needs. "

    # Add energy/sociability text
    if row['Energy_Level'] == 3:
        desc += "Requires lots of space and playtime, definitely **needs an active home**. "
    elif row['Energy_Level'] == 1:
        desc += "Very calm and quiet, perfect for an **apartment-approved** life. "

    # Add family/kid text
    if row['Good_With_Kids'] == 1:
        desc += "**Loves kids** and other pets. "
        tags = ['family-friendly', 'social']
    elif row['Good_With_Kids'] == 2:
        desc += "Needs a home with **experienced, older children**. "
        tags = ['experienced-owner-recommended']
    else:
        desc += "Best suited for a **quiet, adult-only** household. "
        tags = ['adults-only']

    # Add size/other tag
    if row['Size'] == 'Large':
        desc += "Due to its size, a **house with a yard is a must**."
        tags.append('large-breed')

    return desc, tags

nb_data[['Pet_Description', 'Target_Pet_Tags']] = nb_data.apply(
    lambda row: pd.Series(generate_description(row)), axis=1
)

# Finalize data for Naive Bayes
nb_training_data = nb_data[['Pet_ID', 'Pet_Description', 'Target_Pet_Tags']]
nb_training_data.to_csv('3_nb_autotagging_training.csv', index=False)
print("3. Naive Bayes (Auto-Tagging) Training Data saved.")

# =======================================================
# 4. K-Nearest Neighbors (KNN) - Recommendations
# Purpose: Find pets with similar feature vectors.
# Feature Vectors: All pet features (must be numerical)
# =======================================================
knn_data = pet_profiles.copy()
# Convert Pet categorical features into numerical (one-hot or label)
knn_training_data = pd.get_dummies(knn_data.drop(columns=['Pet_ID']), columns=['Type', 'Age_Group', 'Size', 'Grooming_Needs'], prefix='Pet')

# Drop Pet_ID for the final feature matrix, but keep for lookup if needed
knn_training_data = knn_training_data.drop(columns=['Good_With_Kids']).set_index(pet_profiles['Pet_ID'])

# The feature matrix itself is the training data for KNN
knn_training_data.to_csv('4_knn_recommendation_features.csv')
print("4. KNN (Recommendations) Feature Data saved.")

# =======================================================
# 6. Linear Regression (LR) - Adoption Predictions
# Purpose: Predict Days In Shelter.
# Target Variable: Days_In_Shelter (Continuous)
# =======================================================

# Use the pet profiles as the basis and add the target variable
lr_data = pet_profiles.copy()

# A simple rule for Days_In_Shelter (The LR model must learn this relationship):
# Low Energy, Small, Young_Adult pets are adopted fast (low days).
# High Energy, Large, Senior/Puppy pets are adopted slow (high days).
def generate_days_in_shelter(row):
    base_days = 25 # Average days

    if row['Energy_Level'] == 3: base_days += 10 # High energy adds time
    elif row['Energy_Level'] == 1: base_days -= 5 # Low energy reduces time

    if row['Size'] == 'Large': base_days += 15
    elif row['Size'] == 'Small': base_days -= 5

    if row['Age_Group'] == 'Senior' or row['Age_Group'] == 'Puppy/Kitten': base_days += 5
    elif row['Age_Group'] == 'Young_Adult': base_days -= 10

    if row['Grooming_Needs'] == 'High': base_days += 10

    # Add random noise for realism
    noise = np.random.randint(-10, 10)
    final_days = max(5, base_days + noise) # Ensure minimum 5 days
    return final_days

lr_data['Target_Days_In_Shelter'] = lr_data.apply(generate_days_in_shelter, axis=1)

# Encode categorical features for LR
lr_training_data = pd.get_dummies(lr_data.drop(columns=['Pet_ID']), columns=['Type', 'Age_Group', 'Size', 'Grooming_Needs'], drop_first=True)

# Select final columns for LR training
lr_training_data = lr_training_data.drop(columns=['Good_With_Kids']) # Good_With_Kids is less critical for timeline
lr_training_data.to_csv('6_lr_adoption_prediction_training.csv', index=False)
print("6. Linear Regression (Adoption Prediction) Training Data saved.")

print("--- Data Generation Complete ---")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
# Note: SVC stands for Support Vector Classifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
import joblib # For saving the model
import numpy as np # Needed for predicting probabilities

print("--- Starting SVM Model Training (Core Matching) ---")

# 1. Load the Prepared Data
data = pd.read_csv('2_5_svm_ann_matching_training.csv')

# 2. Separate Features (X) and Target (y)
X = data.drop('Success_Match', axis=1)
y = data['Success_Match']

# 3. Split Data into Training and Testing Sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
print(f"Training on {len(X_train)} records, Testing on {len(X_test)} records.")

# 4. Scale Features (Crucial for SVM)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
print("Features scaled successfully.")

# 5. Train the Support Vector Machine Model
# *** CRITICAL CHANGE: probability=True is added here ***
# This enables the model to output a probability (score) instead of just a 0 or 1.
svm_model = SVC(kernel='linear', random_state=42, probability=True)
svm_model.fit(X_train_scaled, y_train)
print("SVM Model training complete.")

# 6. Evaluate the Model
# Use predict() for the traditional classification metrics
y_pred_class = svm_model.predict(X_test_scaled)

# Use predict_proba() to get the actual match score/probability
# probas[:, 1] takes the probability for the "Success" class (Class 1)
y_pred_probas = svm_model.predict_proba(X_test_scaled)
match_scores = np.max(y_pred_probas, axis=1) # The max probability (the confidence score)

print("\n--- Model Evaluation ---")
# Report based on the hard classification
print(f"Accuracy: {accuracy_score(y_test, y_pred_class):.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred_class))

# Report based on the required score output
print("\n--- Example Score Output Check ---")
print(f"Sample Predicted Match Scores (Probabilities): {match_scores[:5]}")
# Example: If the first prediction was 0.92, it would be the '92% match'
print(f"Sample score on first test record (Success Class): {y_pred_probas[0][1]:.2f}")


# 7. Save the trained model and the scaler (needed for deployment)
# The saved model now has the ability to calculate probabilities
joblib.dump(svm_model, 'furever_svm_model.pkl')
joblib.dump(scaler, 'furever_svm_scaler.pkl')
print("\nSVM Model (with probability enabled) and Scaler saved.")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report
import joblib

print("--- Starting Decision Tree Model Training (Pawsonality) ---")

# 1. Load the Prepared Data
dt_data = pd.read_csv('1_dt_pawsonality_training.csv')

# Drop the Adopter_ID as it is not a feature
dt_data = dt_data.drop('Adopter_ID', axis=1)

# 2. Encode Categorical Features
# The target variable (Pawsonality) needs to be converted to numbers (0 to 7)
le_pawsonality = LabelEncoder()
dt_data['Target_Pawsonality_Encoded'] = le_pawsonality.fit_transform(dt_data['Target_Pawsonality'])

# Use One-Hot Encoding for the remaining categorical features (Housing_Type, Experience_Level)
X_encoded = pd.get_dummies(
    dt_data.drop('Target_Pawsonality', axis=1),
    columns=['Housing_Type', 'Experience_Level'],
    drop_first=True
)

# Separate Features (X) and Target (y)
X = X_encoded.drop('Target_Pawsonality_Encoded', axis=1)
y = X_encoded['Target_Pawsonality_Encoded']

# 3. Split Data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 4. Train the Decision Tree Model
dt_model = DecisionTreeClassifier(max_depth=7, random_state=42)
dt_model.fit(X_train, y_train)
print("Decision Tree Model training complete.")

# 5. Evaluate the Model
y_pred = dt_model.predict(X_test)

print("\n--- Model Evaluation ---")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print("\nClassification Report (Pawsonality Types):")
# Display the report using the actual Pawsonality names
target_names = le_pawsonality.classes_
print(classification_report(y_test, y_pred, target_names=target_names))

# 6. Save the trained model and the Pawsonality encoder
joblib.dump(dt_model, 'furever_dt_pawsonality_model.pkl')
joblib.dump(le_pawsonality, 'furever_pawsonality_encoder.pkl')
print("\nDecision Tree Model and Encoder saved.")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, r2_score
import joblib

print("--- Starting Linear Regression Model Training (Adoption Predictions) ---")

# 1. Load the Prepared Data
lr_data = pd.read_csv('6_lr_adoption_prediction_training.csv')

# 2. Separate Features (X) and Target (y)
# Target is 'Target_Days_In_Shelter'
X = lr_data.drop('Target_Days_In_Shelter', axis=1)
y = lr_data['Target_Days_In_Shelter']

# 3. Split Data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 4. Train the Linear Regression Model
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
print("Linear Regression Model training complete.")

# 5. Evaluate the Model
y_pred = lr_model.predict(X_test)

print("\n--- Model Evaluation ---")
# MAE: Shows the average error in days (e.g., predicted 15 days, true was 12 days, error is 3 days)
print(f"Mean Absolute Error (MAE): {mean_absolute_error(y_test, y_pred):.2f} days")
# R2 Score: Measures how well the model explains the variance (closer to 1.0 is better)
print(f"R-squared (R2) Score: {r2_score(y_test, y_pred):.4f}")

# 6. Save the trained model
joblib.dump(lr_model, 'furever_lr_adoption_model.pkl')
print("\nLinear Regression Model saved as 'furever_lr_adoption_model.pkl'.")    

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.multiclass import OneVsRestClassifier
from ast import literal_eval # Used to convert the string representation of a list of tags back into a list
import joblib

print("--- Starting Naive Bayes Model Training (Auto-Tagging) ---")

# 1. Load the Prepared Data
nb_data = pd.read_csv('3_nb_autotagging_training.csv')

# Convert the string representation of lists in Target_Pet_Tags back into actual lists
# 'Target_Pet_Tags' is stored as a string like "['tag1', 'tag2']"
nb_data['Target_Pet_Tags'] = nb_data['Target_Pet_Tags'].apply(literal_eval)

# 2. Separate Features (Text) and Target (Tags)
X = nb_data['Pet_Description']
y_labels = nb_data['Target_Pet_Tags']

# 3. Vectorize the Text Data (Feature Engineering for Text)
# TF-IDF converts text into a numerical matrix, where values represent word importance.
vectorizer = TfidfVectorizer(stop_words='english', max_features=100)
X_vectorized = vectorizer.fit_transform(X)

# 4. Binarize the Tags (Target Encoding for Multi-Label)
# Converts lists of tags into a binary matrix (0 or 1 for each possible tag).
mlb = MultiLabelBinarizer()
y_binarized = mlb.fit_transform(y_labels)

# 5. Split Data (using the vectorized features)
X_train, X_test, y_train, y_test = train_test_split(
    X_vectorized, y_binarized, test_size=0.2, random_state=42
)

# 6. Train the Naive Bayes Model
# OneVsRestClassifier allows a binary classifier (MultinomialNB) to handle multiple labels.
nb_classifier = OneVsRestClassifier(MultinomialNB())
nb_classifier.fit(X_train, y_train)
print("Naive Bayes Model training complete.")

# 7. Evaluate the Model
y_pred = nb_classifier.predict(X_test)
from sklearn.metrics import accuracy_score, jaccard_score

print("\n--- Model Evaluation ---")
# Subset accuracy: The percentage of samples for which all labels were correctly predicted.
print(f"Subset Accuracy: {accuracy_score(y_test, y_pred):.4f}")
# Jaccard score (Mean of Jaccard index for all samples): A good metric for multi-label classification.
print(f"Jaccard Score: {jaccard_score(y_test, y_pred, average='samples'):.4f}")

# 8. Save the trained model and vectorizers/encoders
joblib.dump(nb_classifier, 'furever_nb_autotagging_model.pkl')
joblib.dump(vectorizer, 'furever_nb_vectorizer.pkl')
joblib.dump(mlb, 'furever_nb_mlb.pkl')
print("\nNaive Bayes Model, Vectorizer, and MultiLabelBinarizer saved.")

import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.neighbors import NearestNeighbors
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import joblib

print("--- Starting KNN Model Training (Recommendation Engine) ---")

# 1. Load the Pet Feature Data (This data is different from the text data)
# Corrected filename
knn_data = pd.read_csv('4_knn_recommendation_features.csv')

# Use a Pet ID column to map results back to actual pets.
# We'll drop it before scaling/training, but keep a copy of the index.
# The Pet_ID is the index of the dataframe, so we reset the index to get it as a column
knn_data = knn_data.reset_index()
pet_ids = knn_data['Pet_ID']
knn_data = knn_data.drop('Pet_ID', axis=1) # Drop ID before training

# 2. Define Feature Types (Corrected column names based on generated data)
numerical_features = ['Energy_Level']
categorical_features = ['Pet_Cat', 'Pet_Dog', 'Pet_Puppy/Kitten', 'Pet_Senior', 'Pet_Young_Adult',
                        'Pet_Large', 'Pet_Medium', 'Pet_Small', 'Pet_High', 'Pet_Low', 'Pet_Medium.1']

# 3. Create Preprocessing Pipelines
# This ensures scaling is only applied to numerical data and encoding to categorical data.
# We don't need one-hot encoding here as the categorical features are already one-hot encoded
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        # Removed 'cat' transformer as features are already encoded
    ],
    remainder='passthrough' # Keep any other columns if they exist (these are the one-hot encoded columns)
)


# 4. Apply Preprocessing
# Fit and transform the data
X_processed = preprocessor.fit_transform(knn_data)
print("Data preprocessing (Scaling of numerical features) complete.")

# 5. Train the NearestNeighbors Model
# We are not doing classification here; we are finding neighbors (similarity).
# metric='cosine' is often preferred for high-dimensional similarity.
nn_model = NearestNeighbors(n_neighbors=10, metric='cosine')
nn_model.fit(X_processed)
print(f"NearestNeighbors Model fitted on {X_processed.shape[0]} pets.")

# 6. Save the trained model and the preprocessor
joblib.dump(nn_model, 'furever_nn_recommendations_model.pkl')
joblib.dump(preprocessor, 'furever_nn_preprocessor.pkl')
joblib.dump(pet_ids, 'furever_nn_pet_ids.pkl') # Save the mapping ID list

print("\nKNN Model, Preprocessor, and Pet IDs saved for deployment.")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, f1_score
import joblib

print("--- Starting ANN (MLP) Model Training (Deep Matching) ---")

# 1. Load the Deep Matching Data
# This dataset should contain features for BOTH the pet and the user/home, plus the outcome.
ann_data = pd.read_csv('2_5_svm_ann_matching_training.csv')

# 2. Separate Features (X) and Target (y)
# Target is 'Success_Match' (0 or 1)
X = ann_data.drop('Success_Match', axis=1) # Drop the target column
y = ann_data['Success_Match'] # The binary target (0 or 1)

# 3. Define Feature Types based on the loaded data
# Use the columns from the loaded dataframe
numerical_features = ['Activity_Energy_Diff', 'Has_Kids', 'Good_With_Kids']
categorical_features = [col for col in X.columns if col not in numerical_features]


# 4. Create Preprocessing Pipeline (Scaling and Encoding)
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features), # Scale numerical features
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features) # One-hot encode categorical features
    ],
    remainder='passthrough'
)

# 5. Split Data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 6. Create Model Pipeline (Preprocessor + Classifier)
# The Pipeline ensures that the transformation is applied correctly every time.
ann_classifier = Pipeline(steps=[
    ('preprocessor', preprocessor),
    # Train the MLP Classifier (Neural Network)
    ('classifier', MLPClassifier(
        hidden_layer_sizes=(100, 50), # Two hidden layers
        max_iter=300, # Increased iterations
        random_state=42,
        solver='adam'
    ))
])

# 7. Train the Model
ann_classifier.fit(X_train, y_train)
print("ANN Deep Matching Model training complete.")

# 8. Evaluate the Model
y_pred = ann_classifier.predict(X_test)

print("\n--- ANN Model Evaluation ---")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"F1 Score: {f1_score(y_test, y_pred):.4f}")

# 9. Save the trained model Pipeline
# We save the entire pipeline so the preprocessor is saved with the classifier
joblib.dump(ann_classifier, 'furever_ann_deepmatch_pipeline.pkl')
joblib.dump(X.columns.tolist(), 'furever_ann_feature_names.pkl') # Save original feature list for prediction checks

print("\nANN Deep Match Pipeline and original feature names saved.")

  # --- Update the Load Components Section ---

# Find this section in your testing script and modify the NB model line:
try:
    # 1. NB Auto-Tagging
    # CHANGE THIS LINE to load the tuned model
    nb_classifier = joblib.load('furever_nb_autotagging_model_tuned.pkl')
    nb_vectorizer = joblib.load('furever_nb_vectorizer.pkl')
    nb_mlb = joblib.load('furever_nb_mlb.pkl')

    # ... (Keep the loading for KNN and ANN the same) ...

    print("✅ All model components loaded successfully for testing.")

except FileNotFoundError as e:
    print(f"❌ Error loading file: {e}. Cannot proceed.")


# --- Re-run the Failed Test ---
# Execute the entire testing cell again to re-check TEST 1: Naive Bayes Auto-Tagging

import joblib
import pandas as pd

# --- LOAD ALL COMPONENTS ---
try:
    # 1. NB Auto-Tagging
    nb_classifier = joblib.load('furever_nb_autotagging_model.pkl')
    nb_vectorizer = joblib.load('furever_nb_vectorizer.pkl')
    nb_mlb = joblib.load('furever_nb_mlb.pkl')

    # 2. KNN Recommendations
    nn_model = joblib.load('furever_nn_recommendations_model.pkl')
    nn_preprocessor = joblib.load('furever_nn_preprocessor.pkl')
    pet_ids_list = joblib.load('furever_nn_pet_ids.pkl')

    # 3. ANN Deep Matching
    ann_pipeline = joblib.load('furever_ann_deepmatch_pipeline.pkl')
    ann_feature_names = joblib.load('furever_ann_feature_names.pkl')

    print("✅ All model components loaded successfully for testing.")

except FileNotFoundError as e:
    print(f"❌ Error loading file: {e}. Please ensure all files are saved correctly.")


# ====================================================================
# --- TEST 1: AUTO-TAGGING (NB) ---
# ====================================================================
print("\n--- TEST 1: Naive Bayes Auto-Tagging ---")
# Input: A pet description that should yield known tags
test_desc_1 = "A shy, cuddly senior dog who loves quiet time and is apartment-approved."

# CRITICAL: Use the saved vectorizer's .transform() method
X_test_nb = nb_vectorizer.transform([test_desc_1])
y_pred_nb = nb_classifier.predict(X_test_nb)
predicted_tags = list(nb_mlb.inverse_transform(y_pred_nb)[0])

print(f"Input: '{test_desc_1}'")
print(f"Resulting Tags: {predicted_tags}")
# Simple functional check: ensure 'senior' and 'apartment' tags are present
assert 'senior' in predicted_tags and 'apartment-approved' in predicted_tags, "NB Test Failed: Missing expected tags."
print("TEST 1 Status: Passed functional check.")


# ====================================================================
# --- TEST 2: RECOMMENDATIONS (KNN) ---
# ====================================================================
print("\n--- TEST 2: KNN Recommendations ---")
# Input: Features of a pet (ID 50) you want recommendations for
test_pet_50_features = {
    'Age_Years': 4, 'Weight_Lbs': 50, 'Energy_Level_Score': 8,
    'Species': 'Dog', 'Coat_Type': 'Short', 'Size': 'Medium'
}
test_df_knn = pd.DataFrame([test_pet_50_features])

# 1. Preprocess the features using the saved preprocessor
X_processed_query = nn_preprocessor.transform(test_df_knn)

# 2. Query the model for neighbors (excluding itself, which is the 1st index)
distances, indices = nn_model.kneighbors(X_processed_query, n_neighbors=5)

# 3. Map indices back to Pet IDs
recommended_ids = pet_ids_list.iloc[indices[0]].tolist()
print(f"Query Pet Features: Age 4, Energy 8, Medium Dog")
# The first ID will usually be the pet itself (distance 0), which we usually ignore in a real app
print(f"Recommended Pet IDs: {recommended_ids}")
assert len(recommended_ids) == 5, "KNN Test Failed: Did not return 5 recommendations."
print("TEST 2 Status: Passed functional check (returned expected count).")


# ====================================================================
# --- TEST 3: DEEP MATCHING (ANN) ---
# ====================================================================
print("\n--- TEST 3: ANN Deep Matching ---")
# Input: A \"golden record\" of a strong successful match
test_match_data_good = {
    'Pet_Age_Years': [2], 'User_Income_Score': [8], 'Home_Size_SqFt': [3000],
    'Pet_Energy_Score': [10], 'Pet_Species': ['Dog'],
    'User_Lifestyle_Type': ['Very Active'], 'Home_Location_Type': ['Rural']
}
test_df_ann = pd.DataFrame(test_match_data_good)

# CRITICAL: Reorder columns to match the training data
test_df_ann = test_df_ann[ann_feature_names]

# Predict probability (Score)
match_probability = ann_pipeline.predict_proba(test_df_ann)[:, 1][0]
match_score = int(match_probability * 100)

print(f"User/Pet Match Input: Active User + High-Energy Pet")
print(f"AI DeepMatch Score: {match_score}%")
# Functional check: A good match should be highly probable (> 80%)
assert match_score > 80, f"ANN Test Failed: Score {match_score}% is too low for a good match."
print("TEST 3 Status: Passed functional check (score > 80%).")