{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go985rFRAJhD"
      },
      "source": [
        "**DATASET GENERATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment and versions capture (keeps Colab and server in sync)\n",
        "import sys, json, platform\n",
        "import sklearn, numpy as np, scipy, pandas as pd, joblib\n",
        "\n",
        "versions = {\n",
        "    \"python\": sys.version.split()[0],\n",
        "    \"platform\": platform.platform(),\n",
        "    \"sklearn\": sklearn.__version__,\n",
        "    \"numpy\": np.__version__,\n",
        "    \"scipy\": scipy.__version__,\n",
        "    \"pandas\": pd.__version__,\n",
        "    \"joblib\": joblib.__version__,\n",
        "}\n",
        "print(\"Versions:\", versions)\n",
        "with open('colab_versions.json', 'w') as f:\n",
        "    json.dump(versions, f, indent=2)\n",
        "print(\"Saved to colab_versions.json\")\n",
        "\n",
        "# Optional: warn if versions differ from server pins\n",
        "expected = {\"sklearn\": \"1.6.1\", \"numpy\": \"2.1.3\", \"scipy\": \"1.14.1\", \"pandas\": \"2.2.3\", \"joblib\": \"1.4.2\"}\n",
        "mismatches = []\n",
        "for k, v in expected.items():\n",
        "    have = versions.get(k)\n",
        "    if have and have != v:\n",
        "        mismatches.append(f\"{k}: have {have}, want {v}\")\n",
        "\n",
        "if mismatches:\n",
        "    print(\"Compatibility warning (server expects pinned versions):\")\n",
        "    for m in mismatches:\n",
        "        print(\" -\", m)\n",
        "else:\n",
        "    print(\"Environment matches server pins.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_n4xfsdYAMw9",
        "outputId": "d759bd00-ee17-426f-c615-87361c79f910"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting Data Generation ---\n",
            "Generated and saved pet_profiles.csv with 1000 records.\n",
            "Generated and saved adopter_profiles.csv with 500 records.\n",
            "Generated and saved historical_matches.csv with 2988 records (The Training Data).\n",
            "--- Preparing Training Datasets for 6 ML Algorithms ---\n",
            "1. Decision Tree (Pawsonality) Training Data saved.\n",
            "2 & 5. SVM/ANN (Matching) Training Data saved.\n",
            "3. Naive Bayes (Auto-Tagging) Training Data saved.\n",
            "4. KNN (Recommendations) Feature Data saved.\n",
            "6. Linear Regression (Adoption Prediction) Training Data saved.\n",
            "--- Data Generation Complete ---\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# FurEver.AI - Data Generation (Colab-ready)\n",
        "# ========================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"--- Starting Data Generation ---\")\n",
        "\n",
        "# =======================================================\n",
        "# 1) Generate Pet Profiles (Items)\n",
        "# =======================================================\n",
        "num_pets = 1000\n",
        "\n",
        "pet_data = {\n",
        "    'Pet_ID': [f'P{i:04d}' for i in range(1, num_pets + 1)],\n",
        "    'Type': np.random.choice(['Dog', 'Cat'], size=num_pets, p=[0.6, 0.4]),\n",
        "    'Age_Group': np.random.choice(['Puppy/Kitten', 'Young_Adult', 'Senior'], size=num_pets, p=[0.3, 0.5, 0.2]),\n",
        "    'Size': np.random.choice(['Small', 'Medium', 'Large'], size=num_pets, p=[0.35, 0.45, 0.2]),\n",
        "    # Energy Level: 1=Low, 2=Medium, 3=High (Matches Adopter Activity)\n",
        "    'Energy_Level': np.random.choice([1, 2, 3], size=num_pets, p=[0.3, 0.4, 0.3]),\n",
        "    # Good_With_Kids: 0=No, 1=Yes, 2=Needs older kids (Matches Adopter Has_Kids)\n",
        "    'Good_With_Kids': np.random.choice([0, 1, 2], size=num_pets, p=[0.1, 0.7, 0.2]),\n",
        "    'Grooming_Needs': np.random.choice(['Low', 'Medium', 'High'], size=num_pets, p=[0.5, 0.3, 0.2]),\n",
        "}\n",
        "\n",
        "pet_profiles = pd.DataFrame(pet_data)\n",
        "pet_profiles.to_csv('pet_profiles.csv', index=False)\n",
        "print(f\"Generated and saved pet_profiles.csv with {len(pet_profiles)} records.\")\n",
        "\n",
        "# =======================================================\n",
        "# 2) Generate Adopter Profiles (Users)\n",
        "# =======================================================\n",
        "num_adopters = 500\n",
        "\n",
        "adopter_data = {\n",
        "    'Adopter_ID': [f'U{i:03d}' for i in range(1, num_adopters + 1)],\n",
        "    'Housing_Type': np.random.choice(['Apartment', 'House_No_Yard', 'House_Yard'], size=num_adopters, p=[0.4, 0.3, 0.3]),\n",
        "    'Has_Kids': np.random.choice([0, 1], size=num_adopters, p=[0.6, 0.4]),\n",
        "    # Time_At_Home: 1=Away_Most_Day, 2=Hybrid, 3=WFH_Full_Time\n",
        "    'Time_At_Home': np.random.choice([1, 2, 3], size=num_adopters, p=[0.25, 0.4, 0.35]),\n",
        "    # Activity Level: 1=Low, 2=Medium, 3=High (Matches Pet Energy)\n",
        "    'Activity_Level': np.random.choice([1, 2, 3], size=num_adopters, p=[0.3, 0.4, 0.3]),\n",
        "    'Experience_Level': np.random.choice(['First_Time', 'Past_Owner', 'Expert'], size=num_adopters, p=[0.3, 0.5, 0.2]),\n",
        "    'Pet_Type_Desired': np.random.choice(['Dog', 'Cat'], size=num_adopters, p=[0.7, 0.3]),\n",
        "}\n",
        "\n",
        "adopter_profiles = pd.DataFrame(adopter_data)\n",
        "adopter_profiles.to_csv('adopter_profiles.csv', index=False)\n",
        "print(f\"Generated and saved adopter_profiles.csv with {len(adopter_profiles)} records.\")\n",
        "\n",
        "# =======================================================\n",
        "# 3) Generate Historical Matches (Target for Matching)\n",
        "# =======================================================\n",
        "num_attempts = 3000\n",
        "historical_matches = pd.DataFrame({\n",
        "    'Adopter_ID': np.random.choice(adopter_profiles['Adopter_ID'], size=num_attempts),\n",
        "    'Pet_ID': np.random.choice(pet_profiles['Pet_ID'], size=num_attempts),\n",
        "})\n",
        "\n",
        "# Merge to build rules\n",
        "history = pd.merge(historical_matches, adopter_profiles, on='Adopter_ID')\n",
        "history = pd.merge(history, pet_profiles, on='Pet_ID')\n",
        "\n",
        "# Target\n",
        "history['Success_Match'] = 0\n",
        "\n",
        "# Rule 1: Energy alignment helps\n",
        "history.loc[history['Activity_Level'] == history['Energy_Level'], 'Success_Match'] = 1\n",
        "\n",
        "# Rule 2: Apartment + Large pet hurts\n",
        "history.loc[(history['Housing_Type'] == 'Apartment') & (history['Size'] == 'Large'), 'Success_Match'] = 0\n",
        "\n",
        "# Rule 3: Has kids but pet not good with kids hurts\n",
        "history.loc[(history['Has_Kids'] == 1) & (history['Good_With_Kids'] == 0), 'Success_Match'] = 0\n",
        "\n",
        "# Rule 4: Expert owner + High grooming needs helps\n",
        "history.loc[(history['Experience_Level'] == 'Expert') & (history['Grooming_Needs'] == 'High'), 'Success_Match'] = 1\n",
        "\n",
        "# Add noise\n",
        "noise_count = int(num_attempts * 0.1)\n",
        "history.loc[history.sample(noise_count, random_state=42).index, 'Success_Match'] = np.random.choice([0, 1], size=noise_count)\n",
        "\n",
        "# Finalize training matches\n",
        "historical_matches_final = history[['Adopter_ID', 'Pet_ID', 'Success_Match']].drop_duplicates()\n",
        "historical_matches_final.to_csv('historical_matches.csv', index=False)\n",
        "print(f\"Generated and saved historical_matches.csv with {len(historical_matches_final)} records (The Training Data).\")\n",
        "\n",
        "# =======================================================\n",
        "# 4) Prepare Training Datasets for 6 ML Algorithms\n",
        "# =======================================================\n",
        "print(\"--- Preparing Training Datasets for 6 ML Algorithms ---\")\n",
        "\n",
        "# ---------- 4.1 Decision Tree (Pawsonality) ----------\n",
        "dt_data = adopter_profiles.copy()\n",
        "\n",
        "def classify_pawsonality(row):\n",
        "    # Active Adventurer: High activity & experienced\n",
        "    if row['Activity_Level'] == 3 and row['Experience_Level'] in ['Past_Owner', 'Expert']:\n",
        "        return 'Active Adventurer'\n",
        "    # Cozy Companion: Low activity & Apartment\n",
        "    elif row['Activity_Level'] == 1 and row['Housing_Type'] == 'Apartment':\n",
        "        return 'Cozy Companion'\n",
        "    # Playful Enthusiast: High activity & not always at home\n",
        "    elif row['Activity_Level'] == 3 and row['Time_At_Home'] in [1, 2]:\n",
        "        return 'Playful Enthusiast'\n",
        "    # Confident Guardian: Expert & House with Yard\n",
        "    elif row['Experience_Level'] == 'Expert' and row['Housing_Type'] == 'House_Yard':\n",
        "        return 'Confident Guardian'\n",
        "    # Gentle Nurturer: First/Past owner & WFH full-time\n",
        "    elif row['Experience_Level'] in ['First_Time', 'Past_Owner'] and row['Time_At_Home'] == 3:\n",
        "        return 'Gentle Nurturer'\n",
        "    # Quiet Caretaker: Low activity & not always at home\n",
        "    elif row['Activity_Level'] == 1 and row['Time_At_Home'] in [1, 2]:\n",
        "        return 'Quiet Caretaker'\n",
        "    # Social Butterfly: Medium activity & has kids\n",
        "    elif row['Activity_Level'] == 2 and row['Has_Kids'] == 1:\n",
        "        return 'Social Butterfly'\n",
        "    # Balanced Buddy: default\n",
        "    else:\n",
        "        return 'Balanced Buddy'\n",
        "\n",
        "dt_data['Target_Pawsonality'] = dt_data.apply(classify_pawsonality, axis=1)\n",
        "\n",
        "dt_training_data = dt_data[['Adopter_ID', 'Housing_Type', 'Has_Kids', 'Time_At_Home', 'Activity_Level', 'Experience_Level', 'Target_Pawsonality']]\n",
        "dt_training_data.to_csv('1_dt_pawsonality_training.csv', index=False)\n",
        "print(\"1. Decision Tree (Pawsonality) Training Data saved.\")\n",
        "\n",
        "# ---------- 4.2 + 4.5 SVM & ANN (Core/Deep Matching) ----------\n",
        "# Activity_Energy_Diff and one-hot categorical columns\n",
        "match_df = pd.merge(historical_matches_final, adopter_profiles, on='Adopter_ID')\n",
        "match_df = pd.merge(match_df, pet_profiles, on='Pet_ID')\n",
        "match_df['Activity_Energy_Diff'] = np.abs(match_df['Activity_Level'] - match_df['Energy_Level'])\n",
        "\n",
        "match_features = match_df[['Adopter_ID', 'Pet_ID', 'Activity_Energy_Diff', 'Has_Kids', 'Good_With_Kids', 'Experience_Level', 'Grooming_Needs', 'Size', 'Success_Match']]\n",
        "svm_ann_training_data = pd.get_dummies(match_features, columns=['Experience_Level', 'Grooming_Needs', 'Size'], drop_first=True)\n",
        "svm_ann_training_data = svm_ann_training_data.drop(columns=['Adopter_ID', 'Pet_ID'])\n",
        "\n",
        "svm_ann_training_data.to_csv('2_5_svm_ann_matching_training.csv', index=False)\n",
        "print(\"2 & 5. SVM/ANN (Matching) Training Data saved.\")\n",
        "\n",
        "# ---------- 4.3 Naive Bayes (Auto-Tagging with better rules) ----------\n",
        "nb_data = pet_profiles.copy()\n",
        "\n",
        "def generate_description(row):\n",
        "    # Base description\n",
        "    desc = f\"A {row['Age_Group'].lower().replace('_', ' ')} {row['Type'].lower()} with {row['Grooming_Needs'].lower()} grooming needs. \"\n",
        "    tags = []\n",
        "\n",
        "    # Age-specific tags and text\n",
        "    if row['Age_Group'] == 'Senior':\n",
        "        desc += \"Calm and wise, ideal for a relaxed home. \"\n",
        "        tags.append('senior')\n",
        "    elif row['Age_Group'] == 'Young_Adult':\n",
        "        desc += \"In the prime age for bonding and training. \"\n",
        "        tags.append('young-adult')\n",
        "    elif row['Age_Group'] == 'Puppy/Kitten':\n",
        "        if row['Type'] == 'Dog':\n",
        "            desc += \"Young and learning, needs patience and training. \"\n",
        "            tags.append('puppy')\n",
        "        else:\n",
        "            desc += \"Playful and curious, perfect for a nurturing home. \"\n",
        "            tags.append('kitten')\n",
        "\n",
        "    # Energy and living situation\n",
        "    if row['Energy_Level'] == 3:\n",
        "        desc += \"Requires lots of space and playtime; thrives with an active lifestyle. \"\n",
        "        tags.extend(['high-energy'])\n",
        "        # For large + high-energy we often prefer yard\n",
        "        if row['Size'] == 'Large':\n",
        "            tags.append('house-with-yard')\n",
        "    elif row['Energy_Level'] == 1:\n",
        "        desc += \"Very calm and quiet, perfect for an apartment-approved life. \"\n",
        "        tags.extend(['low-energy', 'apartment-approved'])\n",
        "\n",
        "    # Family/kids compatibility\n",
        "    if row['Good_With_Kids'] == 1:\n",
        "        desc += \"Loves kids and other pets. \"\n",
        "        tags.append('family-friendly')\n",
        "        if row['Energy_Level'] >= 2:\n",
        "            tags.append('social')\n",
        "    elif row['Good_With_Kids'] == 2:\n",
        "        desc += \"Best with experienced families and older children. \"\n",
        "        tags.append('experienced-owner-recommended')\n",
        "    else:\n",
        "        desc += \"Best suited for a quiet, adult-only household. \"\n",
        "        tags.append('adults-only')\n",
        "\n",
        "    # Size-based nuance\n",
        "    if row['Size'] == 'Large':\n",
        "        desc += \"Due to size, a house with a yard is recommended. \"\n",
        "        tags.append('large-breed')\n",
        "\n",
        "    # De-duplicate and return\n",
        "    tags = list(dict.fromkeys(tags))\n",
        "    return desc.strip(), tags\n",
        "\n",
        "nb_data[['Pet_Description', 'Target_Pet_Tags']] = nb_data.apply(\n",
        "    lambda row: pd.Series(generate_description(row)), axis=1\n",
        ")\n",
        "\n",
        "nb_training_data = nb_data[['Pet_ID', 'Pet_Description', 'Target_Pet_Tags']]\n",
        "nb_training_data.to_csv('3_nb_autotagging_training.csv', index=False)\n",
        "print(\"3. Naive Bayes (Auto-Tagging) Training Data saved.\")\n",
        "\n",
        "# ---------- 4.4 KNN (Recommendations) ----------\n",
        "# Keep compatibility with earlier training code that expected duplicated names like 'Pet_Medium.1'\n",
        "knn_data = pet_profiles.copy()\n",
        "knn_training_data = pd.get_dummies(\n",
        "    knn_data.drop(columns=['Pet_ID']),\n",
        "    columns=['Type', 'Age_Group', 'Size', 'Grooming_Needs'],\n",
        "    prefix='Pet'  # Intentional: may create duplicates like Pet_Medium (size) vs Pet_Medium.1 (grooming)\n",
        ")\n",
        "# Drop Good_With_Kids for similarity (optional, matches earlier approach)\n",
        "if 'Good_With_Kids' in knn_training_data.columns:\n",
        "    knn_training_data = knn_training_data.drop(columns=['Good_With_Kids'])\n",
        "# Index by Pet_ID for lookup\n",
        "knn_training_data.index = knn_data['Pet_ID']\n",
        "knn_training_data.to_csv('4_knn_recommendation_features.csv')\n",
        "print(\"4. KNN (Recommendations) Feature Data saved.\")\n",
        "\n",
        "# ---------- 4.6 Linear Regression (Adoption Timeline) ----------\n",
        "lr_data = pet_profiles.copy()\n",
        "\n",
        "def generate_days_in_shelter(row):\n",
        "    base_days = 25\n",
        "    if row['Energy_Level'] == 3: base_days += 10\n",
        "    elif row['Energy_Level'] == 1: base_days -= 5\n",
        "\n",
        "    if row['Size'] == 'Large': base_days += 15\n",
        "    elif row['Size'] == 'Small': base_days -= 5\n",
        "\n",
        "    if row['Age_Group'] in ['Senior', 'Puppy/Kitten']: base_days += 5\n",
        "    elif row['Age_Group'] == 'Young_Adult': base_days -= 10\n",
        "\n",
        "    if row['Grooming_Needs'] == 'High': base_days += 10\n",
        "\n",
        "    noise = np.random.randint(-10, 10)\n",
        "    return max(5, base_days + noise)\n",
        "\n",
        "lr_data['Target_Days_In_Shelter'] = lr_data.apply(generate_days_in_shelter, axis=1)\n",
        "# One-hot for LR training\n",
        "lr_training = pd.get_dummies(\n",
        "    lr_data.drop(columns=['Pet_ID']),\n",
        "    columns=['Type', 'Age_Group', 'Size', 'Grooming_Needs'],\n",
        "    drop_first=True\n",
        ")\n",
        "# Optional: remove Good_With_Kids (as earlier)\n",
        "if 'Good_With_Kids' in lr_training.columns:\n",
        "    lr_training = lr_training.drop(columns=['Good_With_Kids'])\n",
        "\n",
        "lr_training.to_csv('6_lr_adoption_prediction_training.csv', index=False)\n",
        "print(\"6. Linear Regression (Adoption Prediction) Training Data saved.\")\n",
        "\n",
        "print(\"--- Data Generation Complete ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOrtiKN9BFTX"
      },
      "source": [
        "**SVM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TE8OloXXBGkC",
        "outputId": "ca94e1f3-0882-4a00-9eab-480b77284bcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting SVM Model Training (Core Matching) ---\n",
            "Training on 2390 records, Testing on 598 records.\n",
            "Features scaled successfully.\n",
            "SVM Model training complete.\n",
            "\n",
            "--- Model Evaluation ---\n",
            "Accuracy: 0.9013\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.92      0.92       389\n",
            "           1       0.86      0.86      0.86       209\n",
            "\n",
            "    accuracy                           0.90       598\n",
            "   macro avg       0.89      0.89      0.89       598\n",
            "weighted avg       0.90      0.90      0.90       598\n",
            "\n",
            "\n",
            "--- Example Score Output Check ---\n",
            "Sample Predicted Match Scores (Probabilities): [0.85309886 0.80602751 0.99286248 0.992865   0.80594525]\n",
            "Sample score on first test record (Success Class): 0.15\n",
            "\n",
            "SVM Model (with probability enabled) and Scaler saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Note: SVC stands for Support Vector Classifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib # For saving the model\n",
        "import numpy as np # Needed for predicting probabilities\n",
        "\n",
        "print(\"--- Starting SVM Model Training (Core Matching) ---\")\n",
        "\n",
        "# 1. Load the Prepared Data\n",
        "data = pd.read_csv('2_5_svm_ann_matching_training.csv')\n",
        "\n",
        "# 2. Separate Features (X) and Target (y)\n",
        "X = data.drop('Success_Match', axis=1)\n",
        "y = data['Success_Match']\n",
        "\n",
        "# 3. Split Data into Training and Testing Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "print(f\"Training on {len(X_train)} records, Testing on {len(X_test)} records.\")\n",
        "\n",
        "# 4. Scale Features (Crucial for SVM)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "print(\"Features scaled successfully.\")\n",
        "\n",
        "# 5. Train the Support Vector Machine Model\n",
        "# *** CRITICAL CHANGE: probability=True is added here ***\n",
        "# This enables the model to output a probability (score) instead of just a 0 or 1.\n",
        "svm_model = SVC(kernel='linear', random_state=42, probability=True)\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "print(\"SVM Model training complete.\")\n",
        "\n",
        "# 6. Evaluate the Model\n",
        "# Use predict() for the traditional classification metrics\n",
        "y_pred_class = svm_model.predict(X_test_scaled)\n",
        "\n",
        "# Use predict_proba() to get the actual match score/probability\n",
        "# probas[:, 1] takes the probability for the \"Success\" class (Class 1)\n",
        "y_pred_probas = svm_model.predict_proba(X_test_scaled)\n",
        "match_scores = np.max(y_pred_probas, axis=1) # The max probability (the confidence score)\n",
        "\n",
        "print(\"\\n--- Model Evaluation ---\")\n",
        "# Report based on the hard classification\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_class):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_class))\n",
        "\n",
        "# Report based on the required score output\n",
        "print(\"\\n--- Example Score Output Check ---\")\n",
        "print(f\"Sample Predicted Match Scores (Probabilities): {match_scores[:5]}\")\n",
        "# Example: If the first prediction was 0.92, it would be the '92% match'\n",
        "print(f\"Sample score on first test record (Success Class): {y_pred_probas[0][1]:.2f}\")\n",
        "\n",
        "\n",
        "# 7. Save the trained model and the scaler (needed for deployment)\n",
        "# The saved model now has the ability to calculate probabilities\n",
        "joblib.dump(svm_model, 'furever_svm_model.pkl')\n",
        "joblib.dump(scaler, 'furever_svm_scaler.pkl')\n",
        "print(\"\\nSVM Model (with probability enabled) and Scaler saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOVAi7khCAjZ"
      },
      "source": [
        "**DECISION TREES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAUgCqygCErU",
        "outputId": "c92031ea-20bc-4730-f536-1e509ef50286"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting Decision Tree Model Training (Pawsonality) ---\n",
            "Decision Tree Model training complete.\n",
            "\n",
            "--- Model Evaluation ---\n",
            "Accuracy: 0.9900\n",
            "\n",
            "Classification Report (Pawsonality Types):\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            " Active Adventurer       1.00      1.00      1.00        19\n",
            "    Balanced Buddy       1.00      1.00      1.00        19\n",
            "Confident Guardian       1.00      0.75      0.86         4\n",
            "    Cozy Companion       1.00      1.00      1.00        14\n",
            "   Gentle Nurturer       1.00      1.00      1.00        18\n",
            "Playful Enthusiast       1.00      1.00      1.00         5\n",
            "   Quiet Caretaker       0.91      1.00      0.95        10\n",
            "  Social Butterfly       1.00      1.00      1.00        11\n",
            "\n",
            "          accuracy                           0.99       100\n",
            "         macro avg       0.99      0.97      0.98       100\n",
            "      weighted avg       0.99      0.99      0.99       100\n",
            "\n",
            "\n",
            "Decision Tree Model and Encoder saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib\n",
        "\n",
        "print(\"--- Starting Decision Tree Model Training (Pawsonality) ---\")\n",
        "\n",
        "# 1. Load the Prepared Data\n",
        "dt_data = pd.read_csv('1_dt_pawsonality_training.csv')\n",
        "\n",
        "# Drop the Adopter_ID as it is not a feature\n",
        "dt_data = dt_data.drop('Adopter_ID', axis=1)\n",
        "\n",
        "# 2. Encode Categorical Features\n",
        "# The target variable (Pawsonality) needs to be converted to numbers (0 to 7)\n",
        "le_pawsonality = LabelEncoder()\n",
        "dt_data['Target_Pawsonality_Encoded'] = le_pawsonality.fit_transform(dt_data['Target_Pawsonality'])\n",
        "\n",
        "# Use One-Hot Encoding for the remaining categorical features (Housing_Type, Experience_Level)\n",
        "X_encoded = pd.get_dummies(\n",
        "    dt_data.drop('Target_Pawsonality', axis=1),\n",
        "    columns=['Housing_Type', 'Experience_Level'],\n",
        "    drop_first=True\n",
        ")\n",
        "\n",
        "# Separate Features (X) and Target (y)\n",
        "X = X_encoded.drop('Target_Pawsonality_Encoded', axis=1)\n",
        "y = X_encoded['Target_Pawsonality_Encoded']\n",
        "\n",
        "# 3. Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 4. Train the Decision Tree Model\n",
        "dt_model = DecisionTreeClassifier(max_depth=7, random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "print(\"Decision Tree Model training complete.\")\n",
        "\n",
        "# 5. Evaluate the Model\n",
        "y_pred = dt_model.predict(X_test)\n",
        "\n",
        "print(\"\\n--- Model Evaluation ---\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(\"\\nClassification Report (Pawsonality Types):\")\n",
        "# Display the report using the actual Pawsonality names\n",
        "target_names = le_pawsonality.classes_\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "# 6. Save the trained model and the Pawsonality encoder\n",
        "joblib.dump(dt_model, 'furever_dt_pawsonality_model.pkl')\n",
        "joblib.dump(le_pawsonality, 'furever_pawsonality_encoder.pkl')\n",
        "print(\"\\nDecision Tree Model and Encoder saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJm-VqxaDlZf"
      },
      "source": [
        "**LINEAR REGRESSION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnEwinkUDoim",
        "outputId": "fb3f6e63-7344-485b-acfb-8588189e2e8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting Linear Regression Model Training (Adoption Predictions) ---\n",
            "Linear Regression Model training complete.\n",
            "\n",
            "--- Model Evaluation ---\n",
            "Mean Absolute Error (MAE): 5.13 days\n",
            "R-squared (R2) Score: 0.7649\n",
            "\n",
            "Linear Regression Model saved as 'furever_lr_adoption_model.pkl'.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "import joblib\n",
        "\n",
        "print(\"--- Starting Linear Regression Model Training (Adoption Predictions) ---\")\n",
        "\n",
        "# 1. Load the Prepared Data\n",
        "lr_data = pd.read_csv('6_lr_adoption_prediction_training.csv')\n",
        "\n",
        "# 2. Separate Features (X) and Target (y)\n",
        "# Target is 'Target_Days_In_Shelter'\n",
        "X = lr_data.drop('Target_Days_In_Shelter', axis=1)\n",
        "y = lr_data['Target_Days_In_Shelter']\n",
        "\n",
        "# 3. Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Train the Linear Regression Model\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "print(\"Linear Regression Model training complete.\")\n",
        "\n",
        "# 5. Evaluate the Model\n",
        "y_pred = lr_model.predict(X_test)\n",
        "\n",
        "print(\"\\n--- Model Evaluation ---\")\n",
        "# MAE: Shows the average error in days (e.g., predicted 15 days, true was 12 days, error is 3 days)\n",
        "print(f\"Mean Absolute Error (MAE): {mean_absolute_error(y_test, y_pred):.2f} days\")\n",
        "# R2 Score: Measures how well the model explains the variance (closer to 1.0 is better)\n",
        "print(f\"R-squared (R2) Score: {r2_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "# 6. Save the trained model\n",
        "joblib.dump(lr_model, 'furever_lr_adoption_model.pkl')\n",
        "print(\"\\nLinear Regression Model saved as 'furever_lr_adoption_model.pkl'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5XdZZuvDzuY"
      },
      "source": [
        "**NAIVE BAYES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4T8smgeD4dX",
        "outputId": "93819358-3f14-4578-e2e0-ee23344cbfa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting Naive Bayes Model Training (Auto-Tagging) ---\n",
            "Naive Bayes Model training complete.\n",
            "\n",
            "--- Model Evaluation ---\n",
            "Subset Accuracy: 1.0000\n",
            "Jaccard Score: 1.0000\n",
            "\n",
            "Naive Bayes Model, Vectorizer, and MultiLabelBinarizer saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from ast import literal_eval  # Convert stringified lists back to lists\n",
        "import joblib\n",
        "from sklearn.metrics import accuracy_score, jaccard_score\n",
        "\n",
        "print(\"--- Starting Naive Bayes Model Training (Auto-Tagging) ---\")\n",
        "\n",
        "# 1. Load the Prepared Data\n",
        "nb_data = pd.read_csv('3_nb_autotagging_training.csv')\n",
        "\n",
        "# Convert the string representation of lists in Target_Pet_Tags back into actual lists\n",
        "nb_data['Target_Pet_Tags'] = nb_data['Target_Pet_Tags'].apply(literal_eval)\n",
        "\n",
        "# 2. Separate Features (Text) and Target (Tags)\n",
        "X = nb_data['Pet_Description']\n",
        "y_labels = nb_data['Target_Pet_Tags']\n",
        "\n",
        "# Optional light cleanup to remove markdown '**' used in generated text\n",
        "X_clean = X.str.replace('*', '', regex=False)\n",
        "\n",
        "# 3. Vectorize the Text Data (Feature Engineering for Text)\n",
        "# Use richer features: unigrams + bigrams, larger vocab, English stopwords\n",
        "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_features=5000)\n",
        "X_vectorized = vectorizer.fit_transform(X_clean)\n",
        "\n",
        "# 4. Binarize the Tags (Target Encoding for Multi-Label)\n",
        "mlb = MultiLabelBinarizer()\n",
        "y_binarized = mlb.fit_transform(y_labels)\n",
        "\n",
        "# 5. Split Data (using the vectorized features)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_vectorized, y_binarized, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 6. Train the Naive Bayes Model\n",
        "nb_classifier = OneVsRestClassifier(MultinomialNB())\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "print(\"Naive Bayes Model training complete.\")\n",
        "\n",
        "# 7. Evaluate the Model\n",
        "y_pred = nb_classifier.predict(X_test)\n",
        "print(\"\\n--- Model Evaluation ---\")\n",
        "print(f\"Subset Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Jaccard Score: {jaccard_score(y_test, y_pred, average='samples'):.4f}\")\n",
        "\n",
        "# 8. Save the trained model and vectorizers/encoders\n",
        "joblib.dump(nb_classifier, 'furever_nb_autotagging_model.pkl')\n",
        "joblib.dump(vectorizer, 'furever_nb_vectorizer.pkl')\n",
        "joblib.dump(mlb, 'furever_nb_mlb.pkl')\n",
        "print(\"\\nNaive Bayes Model, Vectorizer, and MultiLabelBinarizer saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvCwjzd8EnAM"
      },
      "source": [
        "**KNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZEyBvPOEou4",
        "outputId": "d5255c60-c428-4e06-a972-9d364f9d537e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting KNN Model Training (Recommendation Engine) ---\n",
            "Data preprocessing (Scaling of numerical features) complete.\n",
            "NearestNeighbors Model fitted on 1000 pets.\n",
            "\n",
            "KNN Model, Preprocessor, and Pet IDs saved for deployment.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import joblib\n",
        "\n",
        "print(\"--- Starting KNN Model Training (Recommendation Engine) ---\")\n",
        "\n",
        "# 1. Load the Pet Feature Data (This data is different from the text data)\n",
        "# Corrected filename\n",
        "knn_data = pd.read_csv('4_knn_recommendation_features.csv')\n",
        "\n",
        "# Use a Pet ID column to map results back to actual pets.\n",
        "# We'll drop it before scaling/training, but keep a copy of the index.\n",
        "# The Pet_ID is the index of the dataframe, so we reset the index to get it as a column\n",
        "knn_data = knn_data.reset_index()\n",
        "pet_ids = knn_data['Pet_ID']\n",
        "knn_data = knn_data.drop('Pet_ID', axis=1) # Drop ID before training\n",
        "# Drop the 'index' column that was created by reset_index()\n",
        "knn_data = knn_data.drop('index', axis=1)\n",
        "\n",
        "\n",
        "# 2. Define Feature Types (Corrected column names based on generated data)\n",
        "numerical_features = ['Energy_Level']\n",
        "categorical_features = [col for col in knn_data.columns if col not in numerical_features] # Dynamically determine categorical features\n",
        "\n",
        "# 3. Create Preprocessing Pipelines\n",
        "# This ensures scaling is only applied to numerical data and encoding to categorical data.\n",
        "# We don't need one-hot encoding here as the categorical features are already one-hot encoded\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        # Removed 'cat' transformer as features are already encoded\n",
        "    ],\n",
        "    remainder='passthrough' # Keep any other columns if they exist (these are the one-hot encoded columns)\n",
        ")\n",
        "\n",
        "\n",
        "# 4. Apply Preprocessing\n",
        "# Fit and transform the data\n",
        "X_processed = preprocessor.fit_transform(knn_data)\n",
        "print(\"Data preprocessing (Scaling of numerical features) complete.\")\n",
        "\n",
        "# 5. Train the NearestNeighbors Model\n",
        "# We are not doing classification here; we are finding neighbors (similarity).\n",
        "# metric='cosine' is often preferred for high-dimensional similarity.\n",
        "nn_model = NearestNeighbors(n_neighbors=10, metric='cosine')\n",
        "nn_model.fit(X_processed)\n",
        "print(f\"NearestNeighbors Model fitted on {X_processed.shape[0]} pets.\")\n",
        "\n",
        "# 6. Save the trained model and the preprocessor\n",
        "joblib.dump(nn_model, 'furever_nn_recommendations_model.pkl')\n",
        "joblib.dump(preprocessor, 'furever_nn_preprocessor.pkl')\n",
        "joblib.dump(pet_ids, 'furever_nn_pet_ids.pkl') # Save the mapping ID list\n",
        "\n",
        "print(\"\\nKNN Model, Preprocessor, and Pet IDs saved for deployment.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-jkSL73EySi"
      },
      "source": [
        "**ANN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgWrXUPmE0Tt",
        "outputId": "7a295e98-4124-4ff4-c5aa-b99bcac37a95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting ANN (MLP) Model Training (Deep Matching) ---\n",
            "ANN Deep Matching Model training complete.\n",
            "\n",
            "--- ANN Model Evaluation ---\n",
            "Accuracy: 0.9264\n",
            "F1 Score: 0.8972\n",
            "\n",
            "ANN Deep Match Pipeline and original feature names saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import joblib\n",
        "\n",
        "print(\"--- Starting ANN (MLP) Model Training (Deep Matching) ---\")\n",
        "\n",
        "# 1. Load the Deep Matching Data\n",
        "# This dataset already contains one-hot (dummy) columns created earlier.\n",
        "ann_data = pd.read_csv('2_5_svm_ann_matching_training.csv')\n",
        "\n",
        "# 2. Separate Features (X) and Target (y)\n",
        "X = ann_data.drop('Success_Match', axis=1)\n",
        "y = ann_data['Success_Match']\n",
        "\n",
        "# 3. Define feature types (scale only the true numeric features; pass through dummies)\n",
        "numerical_features = ['Activity_Energy_Diff', 'Has_Kids', 'Good_With_Kids']\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "    ],\n",
        "    remainder='passthrough'  # keep the already-one-hot dummy columns as-is\n",
        ")\n",
        "\n",
        "# 4. Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 5. Create Model Pipeline (Preprocessor + Classifier)\n",
        "ann_classifier = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', MLPClassifier(\n",
        "        hidden_layer_sizes=(100, 50),\n",
        "        max_iter=300,\n",
        "        random_state=42,\n",
        "        solver='adam'\n",
        "    ))\n",
        "])\n",
        "\n",
        "# 6. Train the Model\n",
        "ann_classifier.fit(X_train, y_train)\n",
        "print(\"ANN Deep Matching Model training complete.\")\n",
        "\n",
        "# 7. Evaluate the Model\n",
        "y_pred = ann_classifier.predict(X_test)\n",
        "\n",
        "print(\"\\n--- ANN Model Evaluation ---\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "# 8. Save the trained model Pipeline and original feature list\n",
        "joblib.dump(ann_classifier, 'furever_ann_deepmatch_pipeline.pkl')\n",
        "joblib.dump(X.columns.tolist(), 'furever_ann_feature_names.pkl')\n",
        "\n",
        "print(\"\\nANN Deep Match Pipeline and original feature names saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL2LE0s2Hfgc"
      },
      "source": [
        "**TEST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_2sQBr0JKFq",
        "outputId": "87cd595f-4fd3-43ad-9b91-d1c3e22f6346"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ All model components loaded successfully for testing.\n"
          ]
        }
      ],
      "source": [
        "  # --- Update the Load Components Section ---\n",
        "\n",
        "# Find this section in your testing script and modify the NB model line:\n",
        "try:\n",
        "    # 1. NB Auto-Tagging\n",
        "    # CHANGE THIS LINE to load the tuned model\n",
        "    nb_classifier = joblib.load('furever_nb_autotagging_model.pkl')\n",
        "    nb_vectorizer = joblib.load('furever_nb_vectorizer.pkl')\n",
        "    nb_mlb = joblib.load('furever_nb_mlb.pkl')\n",
        "\n",
        "\n",
        "    # ... (Keep the loading for KNN and ANN the same) ...\n",
        "\n",
        "    print(\"✅ All model components loaded successfully for testing.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"❌ Error loading file: {e}. Cannot proceed.\")\n",
        "\n",
        "\n",
        "# --- Re-run the Failed Test ---\n",
        "# Execute the entire testing cell again to re-check TEST 1: Naive Bayes Auto-Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spFD3WPNHgpm",
        "outputId": "2d3d2b17-8173-42fe-9cbc-e0ca5a7c5c8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ All model components loaded successfully for testing.\n",
            "\n",
            "--- TEST 1: Naive Bayes Auto-Tagging ---\n",
            "Input Description (Generated): 'A senior dog with low grooming needs. Very calm and quiet, perfect for an **apartment-approved** life. Best suited for a **quiet, adult-only** household. '\n",
            "Expected Tags: ['adults-only']\n",
            "Resulting Tags (Predicted): ['adults-only']\n",
            "TEST 1 Status: Passed functional check.\n",
            "\n",
            "--- TEST 2: KNN Recommendations ---\n",
            "Query Pet Feature Vector (Example: High Energy, Young Adult, Medium Dog, Medium Grooming)\n",
            "Recommended Pet IDs: ['P0042', 'P0953', 'P0650', 'P0660', 'P0497']\n",
            "TEST 2 Status: Passed functional check (returned expected count).\n",
            "\n",
            "--- TEST 3: ANN Deep Matching ---\n",
            "User/Pet Match Input (Example: Good compatibility features)\n",
            "AI DeepMatch Score: 95%\n",
            "TEST 3 Status: Passed functional check (score > 80%).\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np  # Import numpy for the generate_description function\n",
        "\n",
        "# --- LOAD ALL COMPONENTS ---\n",
        "try:\n",
        "    # 1. NB Auto-Tagging\n",
        "    nb_classifier = joblib.load('furever_nb_autotagging_model.pkl')\n",
        "    nb_vectorizer = joblib.load('furever_nb_vectorizer.pkl')\n",
        "    nb_mlb = joblib.load('furever_nb_mlb.pkl')\n",
        "\n",
        "    # 2. KNN Recommendations\n",
        "    nn_model = joblib.load('furever_nn_recommendations_model.pkl')\n",
        "    nn_preprocessor = joblib.load('furever_nn_preprocessor.pkl')\n",
        "    pet_ids_list = joblib.load('furever_nn_pet_ids.pkl')\n",
        "\n",
        "    # 3. ANN Deep Matching\n",
        "    ann_pipeline = joblib.load('furever_ann_deepmatch_pipeline.pkl')\n",
        "    ann_feature_names = joblib.load('furever_ann_feature_names.pkl')\n",
        "\n",
        "    print(\"✅ All model components loaded successfully for testing.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"❌ Error loading file: {e}. Please ensure all files are saved correctly.\")\n",
        "\n",
        "\n",
        "# ====================================================================\n",
        "# --- TEST 1: AUTO-TAGGING (NB) ---\n",
        "# ====================================================================\n",
        "print(\"\\n--- TEST 1: Naive Bayes Auto-Tagging ---\")\n",
        "\n",
        "# Replicate the data generation logic for a specific test case\n",
        "\n",
        "def generate_description_for_test(row):\n",
        "    desc = f\"A {row['Age_Group'].lower().replace('_', ' ')} {row['Type'].lower()} with {row['Grooming_Needs'].lower()} grooming needs. \"\n",
        "\n",
        "    # Add energy/sociability text\n",
        "    if row['Energy_Level'] == 3:\n",
        "        desc += \"Requires lots of space and playtime, definitely **needs an active home**. \"\n",
        "    elif row['Energy_Level'] == 1:\n",
        "        desc += \"Very calm and quiet, perfect for an **apartment-approved** life. \"\n",
        "\n",
        "    # Add family/kid text\n",
        "    if row['Good_With_Kids'] == 1:\n",
        "        desc += \"**Loves kids** and other pets. \"\n",
        "        tags = ['family-friendly', 'social']\n",
        "    elif row['Good_With_Kids'] == 2:\n",
        "        desc += \"Needs a home with **experienced, older children**. \"\n",
        "        tags = ['experienced-owner-recommended']\n",
        "    else:  # Good_With_Kids == 0\n",
        "        desc += \"Best suited for a **quiet, adult-only** household. \"\n",
        "        tags = ['adults-only']\n",
        "\n",
        "    # Add size/other tag\n",
        "    if row['Size'] == 'Large':\n",
        "        desc += \"Due to its size, a **house with a yard is a must**.\"\n",
        "        tags.append('large-breed')\n",
        "\n",
        "    # De-duplicate and return\n",
        "    tags = list(dict.fromkeys(tags))\n",
        "    return desc, tags\n",
        "\n",
        "# Define a test pet profile that should generate the desired tags\n",
        "test_pet_profile = pd.Series({\n",
        "    'Age_Group': 'Senior',\n",
        "    'Type': 'Dog',\n",
        "    'Grooming_Needs': 'Low',\n",
        "    'Energy_Level': 1,  # Low energy -> apartment-approved\n",
        "    'Good_With_Kids': 0,  # Not good with kids -> adults-only\n",
        "    'Size': 'Small'  # Doesn't add large-breed tag\n",
        "})\n",
        "\n",
        "# Generate the description and expected tags using the same logic\n",
        "test_desc_1, expected_tags_1 = generate_description_for_test(test_pet_profile)\n",
        "\n",
        "# Clean the description similarly to training cleanup\n",
        "test_desc_1_clean = test_desc_1.replace('*', '')\n",
        "\n",
        "# Use the saved vectorizer's .transform() method\n",
        "X_test_nb = nb_vectorizer.transform([test_desc_1_clean])\n",
        "y_pred_nb = nb_classifier.predict(X_test_nb)\n",
        "predicted_tags = list(nb_mlb.inverse_transform(y_pred_nb)[0])\n",
        "\n",
        "print(f\"Input Description (Generated): '{test_desc_1}'\")\n",
        "print(f\"Expected Tags: {expected_tags_1}\")\n",
        "print(f\"Resulting Tags (Predicted): {predicted_tags}\")\n",
        "\n",
        "# Check if the predicted tags exactly match the expected tags\n",
        "assert sorted(predicted_tags) == sorted(expected_tags_1), f\"NB Test Failed: Predicted tags {predicted_tags} do not match expected tags {expected_tags_1}.\"\n",
        "print(\"TEST 1 Status: Passed functional check.\")\n",
        "\n",
        "\n",
        "# ====================================================================\n",
        "# --- TEST 2: RECOMMENDATIONS (KNN) ---\n",
        "# ====================================================================\n",
        "print(\"\\n--- TEST 2: KNN Recommendations ---\")\n",
        "\n",
        "# Load the exact feature matrix used during training (indexed by Pet_ID)\n",
        "knn_features_df = pd.read_csv('4_knn_recommendation_features.csv', index_col=0)\n",
        "\n",
        "# Pick a real Pet_ID from the fitted model's order if available, otherwise take the first from the CSV\n",
        "candidate_pet_id = pet_ids_list.iloc[0] if hasattr(pet_ids_list, 'iloc') and len(pet_ids_list) > 0 else knn_features_df.index[0]\n",
        "if candidate_pet_id not in knn_features_df.index:\n",
        "    # Fallback to a guaranteed ID\n",
        "    candidate_pet_id = knn_features_df.index[0]\n",
        "\n",
        "query_row = knn_features_df.loc[[candidate_pet_id]]  # keep as DataFrame\n",
        "\n",
        "# Preprocess and query neighbors\n",
        "X_query = nn_preprocessor.transform(query_row)\n",
        "distances, indices = nn_model.kneighbors(X_query, n_neighbors=min(6, len(pet_ids_list)))\n",
        "\n",
        "# Map indices to Pet IDs and drop the query pet itself if present\n",
        "all_ids = pet_ids_list.iloc[indices[0]].tolist()\n",
        "recommended_ids = [pid for pid in all_ids if pid != candidate_pet_id][:5]\n",
        "\n",
        "print(f\"Query Pet_ID: {candidate_pet_id}\")\n",
        "print(f\"Recommended Pet IDs: {recommended_ids}\")\n",
        "assert len(recommended_ids) == 5, \"KNN Test Failed: Did not return 5 recommendations.\"\n",
        "print(\"TEST 2 Status: Passed functional check (returned expected count).\")\n",
        "\n",
        "\n",
        "# ====================================================================\n",
        "# --- TEST 3: DEEP MATCHING (ANN) ---\n",
        "# ====================================================================\n",
        "print(\"\\n--- TEST 3: ANN Deep Matching ---\")\n",
        "\n",
        "# Construct a strong match vector using the exact original training columns\n",
        "# The training CSV already contains the dummy columns; we'll build a minimal row and align.\n",
        "\n",
        "test_match_data_good_vector = pd.DataFrame({\n",
        "    'Activity_Energy_Diff': [0],\n",
        "    'Has_Kids': [1],\n",
        "    'Good_With_Kids': [1],\n",
        "    'Experience_Level_First_Time': [0],\n",
        "    'Experience_Level_Past_Owner': [1],\n",
        "    'Grooming_Needs_Low': [1],\n",
        "    'Grooming_Needs_Medium': [0],\n",
        "    'Size_Medium': [0],\n",
        "    'Size_Small': [1]\n",
        "})\n",
        "\n",
        "# Align to the exact column order expected by the pipeline\n",
        "for col in ann_feature_names:\n",
        "    if col not in test_match_data_good_vector.columns:\n",
        "        test_match_data_good_vector[col] = 0\n",
        "\n",
        "test_df_ann_aligned = test_match_data_good_vector[ann_feature_names]\n",
        "\n",
        "match_probability = ann_pipeline.predict_proba(test_df_ann_aligned)[:, 1][0]\n",
        "match_score = int(match_probability * 100)\n",
        "\n",
        "print(f\"AI DeepMatch Score: {match_score}%\")\n",
        "assert match_score > 80, f\"ANN Test Failed: Score {match_score}% is too low for a good match.\"\n",
        "print(\"TEST 3 Status: Passed functional check (score > 80%).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780
        },
        "id": "IGUqbBdtzh7l",
        "outputId": "feba6d41-3b75-4b6c-be78-008aa2814769"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Checking for model files...\n",
            "✅ Found: furever_dt_pawsonality_model.pkl\n",
            "✅ Found: furever_pawsonality_encoder.pkl\n",
            "✅ Found: furever_svm_model.pkl\n",
            "✅ Found: furever_svm_scaler.pkl\n",
            "✅ Found: furever_nb_autotagging_model.pkl\n",
            "✅ Found: furever_nb_vectorizer.pkl\n",
            "✅ Found: furever_nb_mlb.pkl\n",
            "✅ Found: furever_nn_recommendations_model.pkl\n",
            "✅ Found: furever_nn_preprocessor.pkl\n",
            "✅ Found: furever_nn_pet_ids.pkl\n",
            "✅ Found: furever_ann_deepmatch_pipeline.pkl\n",
            "✅ Found: furever_ann_feature_names.pkl\n",
            "✅ Found: furever_lr_adoption_model.pkl\n",
            "\n",
            "✅ All 13 model files found!\n",
            "\n",
            "📦 Creating furever_models.zip with 13 files...\n",
            "   ✓ Added: furever_dt_pawsonality_model.pkl\n",
            "   ✓ Added: furever_pawsonality_encoder.pkl\n",
            "   ✓ Added: furever_svm_model.pkl\n",
            "   ✓ Added: furever_svm_scaler.pkl\n",
            "   ✓ Added: furever_nb_autotagging_model.pkl\n",
            "   ✓ Added: furever_nb_vectorizer.pkl\n",
            "   ✓ Added: furever_nb_mlb.pkl\n",
            "   ✓ Added: furever_nn_recommendations_model.pkl\n",
            "   ✓ Added: furever_nn_preprocessor.pkl\n",
            "   ✓ Added: furever_nn_pet_ids.pkl\n",
            "   ✓ Added: furever_ann_deepmatch_pipeline.pkl\n",
            "   ✓ Added: furever_ann_feature_names.pkl\n",
            "   ✓ Added: furever_lr_adoption_model.pkl\n",
            "\n",
            "✅ Created furever_models.zip\n",
            "   Size: 232.30 KB\n",
            "\n",
            "📥 Starting download...\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_0345b12c-cd97-40a4-9a27-8c06afe9a89a\", \"furever_models.zip\", 237875)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🎉 Download complete!\n",
            "\n",
            "📋 Next steps:\n",
            "1. Find 'furever_models.zip' in your Downloads folder\n",
            "2. Extract the ZIP file\n",
            "3. Copy all .pkl files to: server/models/\n",
            "4. Run: python server.py\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "print(\"🔍 Checking for model files...\")\n",
        "\n",
        "# List of all required model files\n",
        "model_files = [\n",
        "    'furever_dt_pawsonality_model.pkl',\n",
        "    'furever_pawsonality_encoder.pkl',\n",
        "    'furever_svm_model.pkl',\n",
        "    'furever_svm_scaler.pkl',\n",
        "    'furever_nb_autotagging_model.pkl',\n",
        "    'furever_nb_vectorizer.pkl',\n",
        "    'furever_nb_mlb.pkl',\n",
        "    'furever_nn_recommendations_model.pkl',\n",
        "    'furever_nn_preprocessor.pkl',\n",
        "    'furever_nn_pet_ids.pkl',\n",
        "    'furever_ann_deepmatch_pipeline.pkl',\n",
        "    'furever_ann_feature_names.pkl',\n",
        "    'furever_lr_adoption_model.pkl'\n",
        "]\n",
        "\n",
        "# Check which files exist\n",
        "missing_files = []\n",
        "existing_files = []\n",
        "\n",
        "for file in model_files:\n",
        "    if os.path.exists(file):\n",
        "        existing_files.append(file)\n",
        "        print(f\"✅ Found: {file}\")\n",
        "    else:\n",
        "        missing_files.append(file)\n",
        "        print(f\"❌ Missing: {file}\")\n",
        "\n",
        "if missing_files:\n",
        "    print(f\"\\n⚠️  Warning: {len(missing_files)} model file(s) missing!\")\n",
        "    print(\"Missing files:\")\n",
        "    for f in missing_files:\n",
        "        print(f\"  - {f}\")\n",
        "    print(\"\\nMake sure all training cells completed successfully.\")\n",
        "else:\n",
        "    print(f\"\\n✅ All {len(model_files)} model files found!\")\n",
        "\n",
        "if len(existing_files) > 0:\n",
        "    print(f\"\\n📦 Creating furever_models.zip with {len(existing_files)} files...\")\n",
        "\n",
        "    # Create a ZIP file with all models\n",
        "    with zipfile.ZipFile('furever_models.zip', 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for file in existing_files:\n",
        "            zipf.write(file)\n",
        "            print(f\"   ✓ Added: {file}\")\n",
        "\n",
        "    zip_size = os.path.getsize('furever_models.zip') / 1024\n",
        "    print(f\"\\n✅ Created furever_models.zip\")\n",
        "    print(f\"   Size: {zip_size:.2f} KB\")\n",
        "\n",
        "    print(\"\\n📥 Starting download...\")\n",
        "    files.download('furever_models.zip')\n",
        "\n",
        "    print(\"\\n🎉 Download complete!\")\n",
        "    print(\"\\n📋 Next steps:\")\n",
        "    print(\"1. Find 'furever_models.zip' in your Downloads folder\")\n",
        "    print(\"2. Extract the ZIP file\")\n",
        "    print(\"3. Copy all .pkl files to: server/models/\")\n",
        "    print(\"4. Run: python server.py\")\n",
        "else:\n",
        "    print(\"\\n❌ No model files found. Cannot create ZIP.\")\n",
        "    print(\"Run all training cells in your notebook first.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
