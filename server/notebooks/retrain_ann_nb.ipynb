{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82b403f9",
   "metadata": {},
   "source": [
    "# FurEverAI retrain: Simplified ANN preprocessing and expanded NB vectorizer\n",
    "\n",
    "This notebook retrains two models while preserving the server contract and improving robustness:\n",
    "\n",
    "- ANN Deep Match (predict_proba): a single scikit-learn Pipeline with ColumnTransformer for preprocessing (no double-encoding) and MLPClassifier; exports:\n",
    "  - ann_pipeline.pkl\n",
    "  - ann_features.json (locked feature schema matching get_dummies-style names)\n",
    "  - metrics.json (evaluation)\n",
    "\n",
    "- Naive Bayes Auto-Tag: TF-IDF with expanded n-grams and vocabulary + OneVsRest MultinomialNB; exports:\n",
    "  - nb_vectorizer.pkl\n",
    "  - nb_model.pkl\n",
    "  - tags_schema.json (class order, vocab hash)\n",
    "\n",
    "Artifacts are written to ../models/ so the Flask server can load them directly.\n",
    "\n",
    "Tip: Keep scikit-learn==1.6.1 to match the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63303a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: capture environment and seeds\n",
    "import os, sys, json, hashlib, random, platform\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Print versions\n",
    "print('Python:', sys.version)\n",
    "try:\n",
    "    import sklearn\n",
    "    print('scikit-learn:', sklearn.__version__)\n",
    "except Exception as e:\n",
    "    print('scikit-learn: not installed', e)\n",
    "try:\n",
    "    import pandas as pd\n",
    "    print('pandas:', pd.__version__)\n",
    "except Exception as e:\n",
    "    print('pandas: not installed', e)\n",
    "try:\n",
    "    import torch\n",
    "    print('torch:', torch.__version__)\n",
    "except Exception:\n",
    "    print('torch: not installed (optional)')\n",
    "print('OS:', platform.platform())\n",
    "\n",
    "# Freeze current environment for reproducibility\n",
    "req_path = Path.cwd() / 'requirements_lock.txt'\n",
    "try:\n",
    "    import subprocess\n",
    "    with open(req_path, 'w', encoding='utf-8') as f:\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'freeze'], stdout=f, check=True)\n",
    "    print('Wrote', req_path)\n",
    "except Exception as e:\n",
    "    print('pip freeze failed:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc49d145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and create splits\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths (customize these or drop CSVs in server/notebooks/data/)\n",
    "ROOT = Path.cwd()\n",
    "DATA_DIR = ROOT / 'data'\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "MODELS_DIR = (ROOT / '..' / 'models').resolve()\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Structured ANN dataset expectations (columns); a small synthetic fallback is created if CSV not present.\n",
    "ann_csv = DATA_DIR / 'ann_training.csv'\n",
    "# Expected feature columns\n",
    "NUM_COLS = ['Activity_Level','Has_Kids','Pet_Energy_Level','Pet_Good_With_Kids']\n",
    "CAT_COLS = ['Experience_Level','Pet_Size','Pet_Grooming_Needs']\n",
    "TARGET_COL = 'Match_Label'\n",
    "ALL_COLS = NUM_COLS + CAT_COLS + [TARGET_COL]\n",
    "\n",
    "if ann_csv.exists():\n",
    "    ann_df = pd.read_csv(ann_csv)\n",
    "    missing = [c for c in ALL_COLS if c not in ann_df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f'Missing ANN columns in {ann_csv}: {missing}')\n",
    "    ann_df = ann_df[ALL_COLS].copy()\n",
    "else:\n",
    "    # Synthetic fallback\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    n = 1000\n",
    "    ann_df = pd.DataFrame({\n",
    "        'Activity_Level': rng.integers(1,4,size=n),\n",
    "        'Has_Kids': rng.integers(0,2,size=n),\n",
    "        'Pet_Energy_Level': rng.integers(1,4,size=n),\n",
    "        'Pet_Good_With_Kids': rng.integers(0,3,size=n),\n",
    "        'Experience_Level': rng.choice(['First_Time','Past_Owner','Expert'], size=n),\n",
    "        'Pet_Size': rng.choice(['Small','Medium','Large'], size=n),\n",
    "        'Pet_Grooming_Needs': rng.choice(['Low','Medium','High'], size=n),\n",
    "    })\n",
    "    # Simple target rule + noise\n",
    "    y = ((ann_df['Activity_Level'] >= 2) & (ann_df['Pet_Energy_Level'] >= 2)).astype(int)\n",
    "    y = (y ^ (rng.random(n) < 0.1)).astype(int)\n",
    "    ann_df[TARGET_COL] = y\n",
    "    ann_df.to_csv(ann_csv, index=False)\n",
    "    print('Created synthetic ANN dataset at', ann_csv)\n",
    "\n",
    "X_ann = ann_df[NUM_COLS + CAT_COLS].copy()\n",
    "y_ann = ann_df[TARGET_COL].astype(int).values\n",
    "\n",
    "X_train_ann, X_temp_ann, y_train_ann, y_temp_ann = train_test_split(\n",
    "    X_ann, y_ann, test_size=0.3, random_state=SEED, stratify=y_ann)\n",
    "X_valid_ann, X_test_ann, y_valid_ann, y_test_ann = train_test_split(\n",
    "    X_temp_ann, y_temp_ann, test_size=0.5, random_state=SEED, stratify=y_temp_ann)\n",
    "\n",
    "# Text/Tags dataset for NB (description -> tags); expects nb_training.csv with 'description' and 'tags' (comma-separated).\n",
    "nb_csv = DATA_DIR / 'nb_training.csv'\n",
    "if nb_csv.exists():\n",
    "    nb_df = pd.read_csv(nb_csv)\n",
    "    if 'description' not in nb_df.columns or 'tags' not in nb_df.columns:\n",
    "        raise ValueError('nb_training.csv must have columns: description, tags (comma-separated)')\n",
    "else:\n",
    "    nb_df = pd.DataFrame({\n",
    "        'description': [\n",
    "            'Energetic dog loves running and playing fetch, great with kids in apartments',\n",
    "            'Calm senior cat enjoys quiet naps, perfect for apartment living',\n",
    "            'Playful puppy high energy needs yard and active family',\n",
    "            'Gentle cat independent and calm ideal for small apartment',\n",
    "        ],\n",
    "        'tags': [\n",
    "            'high_energy,good_with_kids,apartment_friendly',\n",
    "            'senior,apartment_friendly,calm',\n",
    "            'high_energy,young,needs_yard',\n",
    "            'calm,independent,apartment_friendly',\n",
    "        ]\n",
    "    })\n",
    "    nb_df.to_csv(nb_csv, index=False)\n",
    "    print('Created synthetic NB dataset at', nb_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871c7ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN preprocessing pipeline: single encoding (no double-encoding)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Build preprocessor\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', scaler, NUM_COLS),\n",
    "        ('cat', ohe, CAT_COLS),\n",
    "    ],\n",
    "    remainder='drop',\n",
    ")\n",
    "\n",
    "# MLP classifier (scikit-learn)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(64,32), activation='relu', solver='adam',\n",
    "                    random_state=SEED, max_iter=300, early_stopping=True, n_iter_no_change=10)\n",
    "\n",
    "ann_pipeline = Pipeline(steps=[('pre', preprocessor), ('clf', mlp)])\n",
    "\n",
    "# Fit only on the train split\n",
    "ann_pipeline.fit(X_train_ann, y_train_ann)\n",
    "valid_acc = ann_pipeline.score(X_valid_ann, y_valid_ann)\n",
    "print('ANN valid accuracy:', round(valid_acc, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975abb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate no double-encoding; derive stable ann_features matching get_dummies style\n",
    "import itertools\n",
    "\n",
    "# Get feature names from transformers (for uniqueness check)\n",
    "ct: ColumnTransformer = ann_pipeline.named_steps['pre']\n",
    "# Feature names with prefixes (e.g., 'num__', 'cat__')\n",
    "feat_prefixed = ct.get_feature_names_out()\n",
    "assert len(set(feat_prefixed)) == len(feat_prefixed), 'Duplicate features after transform'\n",
    "\n",
    "# Derive get_dummies-style names for ann_features.json\n",
    "# numeric -> original names; categorical -> f\"{col}_{category}\"\n",
    "ohe: OneHotEncoder = ct.named_transformers_['cat']\n",
    "cat_feature_names = []\n",
    "for col, cats in zip(CAT_COLS, ohe.categories_):\n",
    "    cat_feature_names.extend([f\"{col}_{str(c)}\" for c in cats])\n",
    "\n",
    "ann_features = NUM_COLS + cat_feature_names\n",
    "assert len(set(ann_features)) == len(ann_features), 'Duplicate names in ann_features'\n",
    "print('ann_features length:', len(ann_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9649addd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist ann_features.json atomically with hash\n",
    "from filelock import FileLock\n",
    "from datetime import datetime\n",
    "\n",
    "features_path = (MODELS_DIR / 'ann_features.json')\n",
    "content = {\n",
    "    'generated_at': datetime.utcnow().isoformat() + 'Z',\n",
    "    'schema': ann_features,\n",
    "}\n",
    "content_bytes = json.dumps(content, ensure_ascii=False, separators=(',',':')).encode('utf-8')\n",
    "sha = hashlib.sha256(content_bytes).hexdigest()\n",
    "content['sha256'] = sha\n",
    "\n",
    "lock = FileLock(str(features_path) + '.lock')\n",
    "with lock:\n",
    "    tmp = features_path.with_suffix('.tmp')\n",
    "    with open(tmp, 'w', encoding='utf-8') as f:\n",
    "        json.dump(content, f, ensure_ascii=False, indent=2)\n",
    "    os.replace(tmp, features_path)\n",
    "print('Wrote', features_path, 'sha256=', sha[:12], '…')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e5687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN evaluation and artifact export\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "\n",
    "# Evaluate\n",
    "y_pred_valid = ann_pipeline.predict(X_valid_ann)\n",
    "y_proba_valid = ann_pipeline.predict_proba(X_valid_ann)[:,1]\n",
    "y_pred_test = ann_pipeline.predict(X_test_ann)\n",
    "y_proba_test = ann_pipeline.predict_proba(X_test_ann)[:,1]\n",
    "\n",
    "metrics_payload = {\n",
    "    'valid': {\n",
    "        'accuracy': float(metrics.accuracy_score(y_valid_ann, y_pred_valid)),\n",
    "        'f1': float(metrics.f1_score(y_valid_ann, y_pred_valid)),\n",
    "        'roc_auc': float(metrics.roc_auc_score(y_valid_ann, y_proba_valid)),\n",
    "    },\n",
    "    'test': {\n",
    "        'accuracy': float(metrics.accuracy_score(y_test_ann, y_pred_test)),\n",
    "        'f1': float(metrics.f1_score(y_test_ann, y_pred_test)),\n",
    "        'roc_auc': float(metrics.roc_auc_score(y_test_ann, y_proba_test)),\n",
    "    },\n",
    "}\n",
    "with open(MODELS_DIR / 'ann_metrics.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics_payload, f, indent=2)\n",
    "print('ANN metrics:', metrics_payload)\n",
    "\n",
    "# Export pipeline\n",
    "ann_pipeline_path = MODELS_DIR / 'ann_pipeline.pkl'\n",
    "joblib.dump(ann_pipeline, ann_pipeline_path)\n",
    "print('Saved', ann_pipeline_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c13bbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB vectorizer expansion (n-grams, features) and training\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Prepare multi-label targets\n",
    "def parse_tags(s: str) -> list:\n",
    "    return [t.strip() for t in (s or '').split(',') if t.strip()]\n",
    "\n",
    "nb_df['tag_list'] = nb_df['tags'].map(parse_tags)\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(nb_df['tag_list'])\n",
    "texts = nb_df['description'].astype(str).tolist()\n",
    "\n",
    "# Split deterministically\n",
    "X_train_txt, X_temp_txt, Y_train, Y_temp = train_test_split(texts, Y, test_size=0.3, random_state=SEED)\n",
    "X_valid_txt, X_test_txt, Y_valid, Y_test = train_test_split(X_temp_txt, Y_temp, test_size=0.5, random_state=SEED)\n",
    "\n",
    "# Vectorizer with richer features\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words='english', max_features=50000, min_df=2, max_df=0.9, sublinear_tf=True)\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train_txt)\n",
    "X_valid_vec = vectorizer.transform(X_valid_txt)\n",
    "X_test_vec = vectorizer.transform(X_test_txt)\n",
    "\n",
    "# NB classifier\n",
    "nb = OneVsRestClassifier(MultinomialNB(alpha=0.5))\n",
    "nb.fit(X_train_vec, Y_train)\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "def eval_nb(X, Y_true, split):\n",
    "    Y_pred = (nb.predict_proba(X) >= 0.5).astype(int)\n",
    "    metrics_dict = {\n",
    "        'micro_f1': float(f1_score(Y_true, Y_pred, average='micro', zero_division=0)),\n",
    "        'macro_f1': float(f1_score(Y_true, Y_pred, average='macro', zero_division=0)),\n",
    "        'micro_precision': float(precision_score(Y_true, Y_pred, average='micro', zero_division=0)),\n",
    "        'micro_recall': float(recall_score(Y_true, Y_pred, average='micro', zero_division=0)),\n",
    "    }\n",
    "    print(split, metrics_dict)\n",
    "    return metrics_dict\n",
    "\n",
    "nb_metrics = {\n",
    "    'valid': eval_nb(X_valid_vec, Y_valid, 'valid'),\n",
    "    'test': eval_nb(X_test_vec, Y_test, 'test'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9771eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: quick hyperparameter search (keep grid tiny to avoid long runs)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline as SkPipeline\n",
    "\n",
    "# Build a pipeline for search (vectorizer + OneVsRest NB)\n",
    "search_pipe = SkPipeline(steps=[\n",
    "    ('vec', TfidfVectorizer(stop_words='english', sublinear_tf=True)),\n",
    "    ('clf', OneVsRestClassifier(MultinomialNB())),\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'vec__ngram_range': [(1,1), (1,2)],\n",
    "    'vec__max_features': [20000, 50000],\n",
    "    'vec__min_df': [1, 2],\n",
    "    'clf__estimator__alpha': [1.0, 0.5, 0.25],\n",
    "}\n",
    "\n",
    "# Use a very small CV for speed; scoring macro F1\n",
    "search = GridSearchCV(search_pipe, param_grid, scoring='f1_macro', cv=3, n_jobs=-1, verbose=1)\n",
    "search.fit(X_train_txt, Y_train)\n",
    "print('Best params:', search.best_params_)\n",
    "print('Best score:', search.best_score_)\n",
    "\n",
    "# Refit a final model using best params (and then export vectorizer + model separately)\n",
    "best_vec: TfidfVectorizer = TfidfVectorizer(stop_words='english', sublinear_tf=True,\n",
    "                                            ngram_range=search.best_params_['vec__ngram_range'],\n",
    "                                            max_features=search.best_params_['vec__max_features'],\n",
    "                                            min_df=search.best_params_['vec__min_df'])\n",
    "X_train_vec2 = best_vec.fit_transform(X_train_txt)\n",
    "X_valid_vec2 = best_vec.transform(X_valid_txt)\n",
    "X_test_vec2 = best_vec.transform(X_test_txt)\n",
    "\n",
    "nb2 = OneVsRestClassifier(MultinomialNB(alpha=search.best_params_['clf__estimator__alpha']))\n",
    "nb2.fit(X_train_vec2, Y_train)\n",
    "\n",
    "print('Re-evaluating with best params:')\n",
    "_ = eval_nb(X_valid_vec2, Y_valid, 'valid(best)')\n",
    "_ = eval_nb(X_test_vec2, Y_test, 'test(best)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5964a55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export NB model and vectorizer\n",
    "import joblib as _joblib\n",
    "\n",
    "# Choose the best refit if available, else the first one\n",
    "vec_to_save = best_vec if 'best_vec' in globals() else vectorizer\n",
    "nb_to_save = nb2 if 'nb2' in globals() else nb\n",
    "\n",
    "# Save artifacts\n",
    "vec_path = MODELS_DIR / 'nb_vectorizer.pkl'\n",
    "nb_path = MODELS_DIR / 'nb_model.pkl'\n",
    "_joblib.dump(vec_to_save, vec_path)\n",
    "_joblib.dump(nb_to_save, nb_path)\n",
    "\n",
    "# Save schema\n",
    "tags_schema = {\n",
    "    'classes': list(mlb.classes_),\n",
    "    'vocab_size': int(getattr(vec_to_save, 'vocabulary_', {}) and len(vec_to_save.vocabulary_) or 0),\n",
    "}\n",
    "with open(MODELS_DIR / 'tags_schema.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(tags_schema, f, indent=2)\n",
    "print('Saved NB artifacts to', MODELS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e67796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version parity check: write out colab_versions.json (compare manually to server)\n",
    "def get_versions():\n",
    "    out = {'python': sys.version.split()[0], 'platform': platform.platform()}\n",
    "    try:\n",
    "        import sklearn; out['scikit_learn'] = sklearn.__version__\n",
    "    except Exception:\n",
    "        out['scikit_learn'] = None\n",
    "    try:\n",
    "        import numpy as _np; out['numpy'] = _np.__version__\n",
    "    except Exception:\n",
    "        out['numpy'] = None\n",
    "    try:\n",
    "        import pandas as _pd; out['pandas'] = _pd.__version__\n",
    "    except Exception:\n",
    "        out['pandas'] = None\n",
    "    try:\n",
    "        import scipy as _sc; out['scipy'] = _sc.__version__\n",
    "    except Exception:\n",
    "        out['scipy'] = None\n",
    "    try:\n",
    "        import torch as _th; out['torch'] = _th.__version__\n",
    "    except Exception:\n",
    "        out['torch'] = None\n",
    "    return out\n",
    "\n",
    "colab_versions = get_versions()\n",
    "with open(MODELS_DIR / 'colab_versions.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(colab_versions, f, indent=2)\n",
    "print('colab_versions:', colab_versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6b95f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export steps and artifact hashing\n",
    "def sha256sum(p: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(p, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b''):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "artifacts = [\n",
    "    MODELS_DIR / 'ann_pipeline.pkl',\n",
    "    MODELS_DIR / 'ann_features.json',\n",
    "    MODELS_DIR / 'ann_metrics.json',\n",
    "    MODELS_DIR / 'nb_vectorizer.pkl',\n",
    "    MODELS_DIR / 'nb_model.pkl',\n",
    "    MODELS_DIR / 'tags_schema.json',\n",
    "    MODELS_DIR / 'colab_versions.json',\n",
    "]\n",
    "\n",
    "checksum_path = MODELS_DIR / 'checksums.sha256'\n",
    "with open(checksum_path, 'w', encoding='utf-8') as f:\n",
    "    for p in artifacts:\n",
    "        if p.exists():\n",
    "            s = sha256sum(p)\n",
    "            f.write(f\"{s}  {p.name}\\n\")\n",
    "print('Wrote checksums to', checksum_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aed072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VS Code-style sanity tests (assertions)\n",
    "# 1) No double-encoding\n",
    "assert len(set(ann_features)) == len(ann_features), 'Duplicate in ann_features'\n",
    "\n",
    "# 2) ANN pipeline predict_proba shape\n",
    "proba_shape = ann_pipeline.predict_proba(X_valid_ann).shape\n",
    "assert proba_shape[1] == 2, f'Expected binary proba, got shape {proba_shape}'\n",
    "\n",
    "# 3) Vectorizer reproducibility\n",
    "vec2 = TfidfVectorizer(ngram_range=vec_to_save.ngram_range, stop_words='english', max_features=vec_to_save.max_features, min_df=vec_to_save.min_df, max_df=vec_to_save.max_df, sublinear_tf=True)\n",
    "vec2.fit(X_train_txt)\n",
    "assert vec2.get_feature_names_out().shape == vec_to_save.get_feature_names_out().shape, 'Vectorizer feature size mismatch'\n",
    "print('All sanity tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733e232c",
   "metadata": {},
   "source": [
    "## How to run\n",
    "\n",
    "1. Place your CSVs (optional) under `server/notebooks/data/`:\n",
    "   - `ann_training.csv` with columns: Activity_Level, Has_Kids, Pet_Energy_Level, Pet_Good_With_Kids, Experience_Level, Pet_Size, Pet_Grooming_Needs, Match_Label\n",
    "   - `nb_training.csv` with columns: description, tags (comma-separated)\n",
    "   If not provided, the notebook will create small synthetic datasets for demonstration.\n",
    "\n",
    "2. Ensure versions match the server (recommended): scikit-learn==1.6.1\n",
    "   You can install inside the notebook if needed:\n",
    "   - In a cell: `%pip install scikit-learn==1.6.1 pandas==2.2.3 numpy==2.1.3 scipy==1.14.1 joblib==1.4.2`\n",
    "\n",
    "3. Run cells 1→end. Artifacts will be written to `server/models/`:\n",
    "   - ann_pipeline.pkl, ann_features.json, ann_metrics.json\n",
    "   - nb_vectorizer.pkl, nb_model.pkl, tags_schema.json\n",
    "   - colab_versions.json, checksums.sha256\n",
    "\n",
    "4. Restart the Flask server (if it’s running) so it reloads the new models.\n",
    "\n",
    "Notes:\n",
    "- ann_features.json is deliberately get_dummies-style (no ColumnTransformer prefixes) to stay compatible with the server’s `get_dummies` alignment.\n",
    "- The ANN uses scikit-learn MLPClassifier to preserve the `predict_proba` contract and avoid extra wrappers.\n",
    "- The NB pipeline expands to 1–2 grams and a larger vocab for richer tags; tune the grid if you have more data.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
